{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Begin amigos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsArticle:\n",
    "    def __init__(self, title: str, content: str, date: str, source: str, url: str):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.date = date\n",
    "        self.source = source\n",
    "        self.url = url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialNewsScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        self.sources = {\n",
    "            'reuters': 'https://www.reuters.com/markets/companies',\n",
    "            'marketwatch': 'https://www.marketwatch.com/markets',\n",
    "            'investing': 'https://www.investing.com/news/stock-market-news'\n",
    "        }\n",
    "\n",
    "    def scrape_news(self, days_back: int = 7) -> List[NewsArticle]:\n",
    "        articles = []\n",
    "        for source, url in self.sources.items():\n",
    "            try:\n",
    "                articles.extend(self._scrape_source(source, url, days_back))\n",
    "                time.sleep(2)  # Polite delay between sources\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping {source}: {str(e)}\")\n",
    "        return articles\n",
    "\n",
    "    def _scrape_source(self, source: str, url: str, days_back: int) -> List[NewsArticle]:\n",
    "        articles = []\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            if source == 'reuters':\n",
    "                articles.extend(self._parse_reuters(soup))\n",
    "            elif source == 'marketwatch':\n",
    "                articles.extend(self._parse_marketwatch(soup))\n",
    "            elif source == 'investing':\n",
    "                articles.extend(self._parse_investing(soup))\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in _scrape_source for {source}: {str(e)}\")\n",
    "            \n",
    "        return articles\n",
    "\n",
    "    def _parse_reuters(self, soup: BeautifulSoup) -> List[NewsArticle]:\n",
    "        articles = []\n",
    "        for article in soup.find_all('article'):\n",
    "            try:\n",
    "                title = article.find('h3').text.strip()\n",
    "                link = article.find('a')['href']\n",
    "                article_content = self._get_article_content(f\"https://reuters.com{link}\")\n",
    "                date = datetime.now().strftime('%Y-%m-%d')  # Reuters articles usually have current date\n",
    "                articles.append(NewsArticle(title, article_content, date, 'Reuters', link))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error parsing Reuters article: {str(e)}\")\n",
    "        return articles\n",
    "\n",
    "    def _get_article_content(self, url: str) -> str:\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            return ' '.join([p.text.strip() for p in paragraphs])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting article content: {str(e)}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, sequence_length: int, target_column: int):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.sequence_length]\n",
    "        y = self.data[idx + self.sequence_length][self.target_column]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class StockPredictor:\n",
    "    def __init__(self,\n",
    "                 time_series_model_name: str = \"huggingface/TimeSeriesTransformer\",\n",
    "                 sentiment_model_name: str = \"ProsusAI/finbert\",\n",
    "                 api_access_token: str = \"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\"):\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load time series model for regression\n",
    "        self.time_series_model = AutoModel.from_pretrained(time_series_model_name, token=api_access_token).to(self.device)\n",
    "\n",
    "        # Load sentiment analysis model\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name, token=api_access_token).to(self.device)\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name, token=api_access_token)\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def load_data_from_csv(self, file_path: str):\n",
    "        \"\"\"Load stock data from a CSV file and split into features (X) and target (y)\"\"\"\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Standardize column names (remove leading/trailing spaces)\n",
    "            df.columns = df.columns.str.strip()\n",
    "\n",
    "            # Expected columns\n",
    "            expected_columns = [\n",
    "                \"date\", \"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\",\n",
    "                \"EMA\", \"Volume_Oscillator\", \"RSI\", \"%K\", \"%D\", \"+DI\", \"-DI\", \"ADX\", \"PVT\", \"Target\"\n",
    "            ]\n",
    "\n",
    "            missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in CSV file: {missing_cols}\")\n",
    "\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "            df.dropna(subset=[\"date\"], inplace=True)\n",
    "            df.set_index(\"date\", inplace=True)\n",
    "\n",
    "            df[expected_columns[1:]] = df[expected_columns[1:]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "            X = df.drop(columns=[\"Target\"]).values  # Features\n",
    "            y = df[\"Target\"].values  # Target variable\n",
    "\n",
    "            return X, y\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def prepare_data_for_training(self, X, y, test_size=0.2, batch_size=32):\n",
    "        \"\"\"Prepare PyTorch data loaders\"\"\"\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Normalize features\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val = self.scaler.transform(X_val)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train_model(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 10):\n",
    "        \"\"\"Train the transformer model for time series regression\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.time_series_model.parameters(), lr=1e-4)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.time_series_model.train()\n",
    "            train_loss = 0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                batch_x, batch_y = batch\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.time_series_model(batch_x).logits  # Ensure model outputs logits\n",
    "                outputs = outputs.squeeze()\n",
    "\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "    def analyze_sentiment(self, text: str):\n",
    "        \"\"\"Analyze sentiment of financial news\"\"\"\n",
    "        inputs = self.sentiment_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.sentiment_model(**inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            prediction = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "        return prediction.item(), probabilities.cpu().numpy()\n",
    "\n",
    "    def predict_stock_price(self, X):\n",
    "        \"\"\"Predict stock price using trained model\"\"\"\n",
    "        self.time_series_model.eval()\n",
    "\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.time_series_model(X_tensor).logits.squeeze().cpu().numpy()\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in main execution:too many values to unpack (expected 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred. Check logs for details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    \n",
    "    predictor_file_path = r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\features.csv\"\n",
    "    predictor = StockPredictor()\n",
    "\n",
    "    try:\n",
    "        # Load stock data\n",
    "        X, y = predictor.load_data_from_csv(predictor_file_path)\n",
    "\n",
    "        if X is None or y is None:\n",
    "            logging.error(\"Failed to load data.\")\n",
    "            return\n",
    "\n",
    "        # Prepare data loaders\n",
    "        train_loader, val_loader = predictor.prepare_data_for_training(X, y)\n",
    "\n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        predictor.train_model(train_loader, val_loader)\n",
    "\n",
    "        # Predict stock price\n",
    "        X_test = X[-10:]  # Example: last 10 rows for prediction\n",
    "        predictions = predictor.predict_stock_price(X_test)\n",
    "\n",
    "        print(f\"\\nPredicted Stock Prices: {predictions}\")\n",
    "\n",
    "        # Sentiment analysis example\n",
    "        sentiment_result = predictor.analyze_sentiment(\"The stock market is experiencing a strong bullish trend.\")\n",
    "        print(f\"\\nSentiment Analysis: {sentiment_result}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {str(e)}\")\n",
    "        print(\"An error occurred. Check logs for details.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
