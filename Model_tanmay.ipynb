{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Begin amigos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import (\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModel,\n",
    "# )\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# # from datetime import datetime, timedelta\n",
    "# import logging\n",
    "# import time\n",
    "# from typing import List, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NewsArticle:\n",
    "#     def __init__(self, title: str, content: str, date: str, source: str, url: str):\n",
    "#         self.title = title\n",
    "#         self.content = content\n",
    "#         self.date = date\n",
    "#         self.source = source\n",
    "#         self.url = url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FinancialNewsScraper:\n",
    "#     def __init__(self):\n",
    "#         self.headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "#         }\n",
    "#         self.sources = {\n",
    "#             'reuters': 'https://www.reuters.com/markets/companies',\n",
    "#             'marketwatch': 'https://www.marketwatch.com/markets',\n",
    "#             'investing': 'https://www.investing.com/news/stock-market-news'\n",
    "#         }\n",
    "\n",
    "#     def scrape_news(self, days_back: int = 7) -> List[NewsArticle]:\n",
    "#         articles = []\n",
    "#         for source, url in self.sources.items():\n",
    "#             try:\n",
    "#                 articles.extend(self._scrape_source(source, url, days_back))\n",
    "#                 time.sleep(2)  # Polite delay between sources\n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error scraping {source}: {str(e)}\")\n",
    "#         return articles\n",
    "\n",
    "#     def _scrape_source(self, source: str, url: str, days_back: int) -> List[NewsArticle]:\n",
    "#         articles = []\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=self.headers)\n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "#             if source == 'reuters':\n",
    "#                 articles.extend(self._parse_reuters(soup))\n",
    "#             elif source == 'marketwatch':\n",
    "#                 articles.extend(self._parse_marketwatch(soup))\n",
    "#             elif source == 'investing':\n",
    "#                 articles.extend(self._parse_investing(soup))\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in _scrape_source for {source}: {str(e)}\")\n",
    "            \n",
    "#         return articles\n",
    "\n",
    "#     def _parse_reuters(self, soup: BeautifulSoup) -> List[NewsArticle]:\n",
    "#         articles = []\n",
    "#         for article in soup.find_all('article'):\n",
    "#             try:\n",
    "#                 title = article.find('h3').text.strip()\n",
    "#                 link = article.find('a')['href']\n",
    "#                 article_content = self._get_article_content(f\"https://reuters.com{link}\")\n",
    "#                 date = datetime.now().strftime('%Y-%m-%d')  # Reuters articles usually have current date\n",
    "#                 articles.append(NewsArticle(title, article_content, date, 'Reuters', link))\n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error parsing Reuters article: {str(e)}\")\n",
    "#         return articles\n",
    "\n",
    "#     def _get_article_content(self, url: str) -> str:\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=self.headers)\n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#             paragraphs = soup.find_all('p')\n",
    "#             return ' '.join([p.text.strip() for p in paragraphs])\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error getting article content: {str(e)}\")\n",
    "#             return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FinancialDataset(Dataset):\n",
    "#     def __init__(self, data: np.ndarray, sequence_length: int, target_column: int):\n",
    "#         self.data = torch.FloatTensor(data)\n",
    "#         self.sequence_length = sequence_length\n",
    "#         self.target_column = target_column\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data) - self.sequence_length\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         x = self.data[idx:idx + self.sequence_length]\n",
    "#         y = self.data[idx + self.sequence_length][self.target_column]\n",
    "#         return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import logging\n",
    "# import yfinance as yf\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# class StockPredictor:\n",
    "#     def __init__(self,\n",
    "#                  time_series_model_name: str = \"huggingface/TimeSeriesTransformer\",\n",
    "#                  sentiment_model_name: str = \"ProsusAI/finbert\",\n",
    "#                  api_access_token: str = \"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\"):\n",
    "        \n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#         # Load time series model for regression\n",
    "#         self.time_series_model = AutoModel.from_pretrained(time_series_model_name, token=api_access_token).to(self.device)\n",
    "\n",
    "#         # Load sentiment analysis model\n",
    "#         self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name, token=api_access_token).to(self.device)\n",
    "#         self.sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name, token=api_access_token)\n",
    "        \n",
    "#         # Initialize scaler\n",
    "#         self.scaler = MinMaxScaler()\n",
    "\n",
    "#     def load_data_from_csv(self, file_path: str):\n",
    "#         \"\"\"Load stock data from a CSV file and split into features (X) and target (y)\"\"\"\n",
    "\n",
    "#         try:\n",
    "#             df = pd.read_csv(file_path)\n",
    "\n",
    "#             # Standardize column names (remove leading/trailing spaces)\n",
    "#             df.columns = df.columns.str.strip()\n",
    "\n",
    "#             # Expected columns\n",
    "#             expected_columns = [\n",
    "#                 \"date\", \"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\",\n",
    "#                 \"EMA\", \"Volume_Oscillator\", \"RSI\", \"%K\", \"%D\", \"+DI\", \"-DI\", \"ADX\", \"PVT\", \"Target\"\n",
    "#             ]\n",
    "\n",
    "#             missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "#             if missing_cols:\n",
    "#                 raise ValueError(f\"Missing columns in CSV file: {missing_cols}\")\n",
    "\n",
    "#             df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "#             df.dropna(subset=[\"date\"], inplace=True)\n",
    "#             df.set_index(\"date\", inplace=True)\n",
    "\n",
    "#             df[expected_columns[1:]] = df[expected_columns[1:]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "#             X = df.drop(columns=[\"Target\"]).values  # Features\n",
    "#             y = df[\"Target\"].values  # Target variable\n",
    "\n",
    "#             return X, y\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading CSV: {e}\")\n",
    "#             return None, None\n",
    "\n",
    "#     def prepare_data_for_training(self, X, y, test_size=0.2, batch_size=32):\n",
    "#         \"\"\"Prepare PyTorch data loaders\"\"\"\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "#         # Normalize features\n",
    "#         X_train = self.scaler.fit_transform(X_train)\n",
    "#         X_val = self.scaler.transform(X_val)\n",
    "\n",
    "#         # Convert to tensors\n",
    "#         X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "#         y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
    "#         X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)\n",
    "#         y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
    "\n",
    "#         train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#         val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         return train_loader, val_loader\n",
    "\n",
    "#     def train_model(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 10):\n",
    "#         \"\"\"Train the transformer model for time series regression\"\"\"\n",
    "#         optimizer = torch.optim.Adam(self.time_series_model.parameters(), lr=1e-4)\n",
    "#         criterion = torch.nn.MSELoss()\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             self.time_series_model.train()\n",
    "#             train_loss = 0\n",
    "\n",
    "#             for batch in train_loader:\n",
    "#                 batch_x, batch_y = batch\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 outputs = self.time_series_model(batch_x).logits  # Ensure model outputs logits\n",
    "#                 outputs = outputs.squeeze()\n",
    "\n",
    "#                 loss = criterion(outputs, batch_y)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 train_loss += loss.item()\n",
    "\n",
    "#             print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "#     def analyze_sentiment(self, text: str):\n",
    "#         \"\"\"Analyze sentiment of financial news\"\"\"\n",
    "#         inputs = self.sentiment_tokenizer(\n",
    "#             text,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=128\n",
    "#         ).to(self.device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.sentiment_model(**inputs)\n",
    "#             probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "#             prediction = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "#         return prediction.item(), probabilities.cpu().numpy()\n",
    "\n",
    "#     def predict_stock_price(self, X):\n",
    "#         \"\"\"Predict stock price using trained model\"\"\"\n",
    "#         self.time_series_model.eval()\n",
    "\n",
    "#         X_scaled = self.scaler.transform(X)\n",
    "#         X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             predictions = self.time_series_model(X_tensor).logits.squeeze().cpu().numpy()\n",
    "\n",
    "#         return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main():\n",
    "#     logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    \n",
    "#     predictor_file_path = r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\features.csv\"\n",
    "#     predictor = StockPredictor()\n",
    "\n",
    "#     try:\n",
    "#         # Load stock data\n",
    "#         X, y = predictor.load_data_from_csv(predictor_file_path)\n",
    "\n",
    "#         if X is None or y is None:\n",
    "#             logging.error(\"Failed to load data.\")\n",
    "#             return\n",
    "\n",
    "#         # Prepare data loaders\n",
    "#         train_loader, val_loader = predictor.prepare_data_for_training(X, y)\n",
    "\n",
    "#         # Train model\n",
    "#         print(\"Training model...\")\n",
    "#         predictor.train_model(train_loader, val_loader)\n",
    "\n",
    "#         # Predict stock price\n",
    "#         X_test = X[-10:]  # Example: last 10 rows for prediction\n",
    "#         predictions = predictor.predict_stock_price(X_test)\n",
    "\n",
    "#         print(f\"\\nPredicted Stock Prices: {predictions}\")\n",
    "\n",
    "#         # Sentiment analysis example\n",
    "#         sentiment_result = predictor.analyze_sentiment(\"The stock market is experiencing a strong bullish trend.\")\n",
    "#         print(f\"\\nSentiment Analysis: {sentiment_result}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error in main execution: {str(e)}\")\n",
    "#         print(\"An error occurred. Check logs for details.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Supervised Contrastive Loss (SCL)\n",
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(SupervisedContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Compute the supervised contrastive loss.\n",
    "        Arguments:\n",
    "            features: Tensor of shape (batch_size, feature_dim)\n",
    "            labels: Tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # Normalize features\n",
    "        features = nn.functional.normalize(features, p=2, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity between all pairs of features\n",
    "        similarity_matrix = torch.matmul(features, features.T)\n",
    "        labels = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "        \n",
    "        # Compute the contrastive loss\n",
    "        logits = similarity_matrix / self.temperature\n",
    "        labels = labels.float()\n",
    "        \n",
    "        loss = torch.mean(torch.sum(-labels * nn.functional.log_softmax(logits, dim=-1), dim=-1))\n",
    "        return loss\n",
    "\n",
    "class StockPredictor:\n",
    "    def __init__(self):\n",
    "        # Initialize scaler for normalization\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.01)\n",
    "\n",
    "        # Neural network for contrastive learning\n",
    "        self.neural_net = nn.Sequential(\n",
    "            nn.Linear(15, 64),  # 14 features in total (date is not included as it is not a numerical feature for regression)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.criterion = SupervisedContrastiveLoss()\n",
    "\n",
    "    def load_data_from_csv(self, file_path: str):\n",
    "        \"\"\"Load stock data from a CSV file and split into features (X) and target (y)\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Standardize column names (remove leading/trailing spaces)\n",
    "            df.columns = df.columns.str.strip()\n",
    "\n",
    "            # Expected columns\n",
    "            expected_columns = [\n",
    "                \"date\", \"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\",\n",
    "                \"EMA\", \"Volume_Oscillator\", \"RSI\", \"%K\", \"%D\", \"+DI\", \"-DI\", \"ADX\", \"PVT\", \"Target\"\n",
    "            ]\n",
    "\n",
    "            missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in CSV file: {missing_cols}\")\n",
    "\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "            df.dropna(subset=[\"date\"], inplace=True)\n",
    "            df.set_index(\"date\", inplace=True)\n",
    "\n",
    "            df[expected_columns[1:]] = df[expected_columns[1:]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "            X = df.drop(columns=[\"Target\"]).values  # Features\n",
    "            y = df[\"Target\"].values  # Target variable\n",
    "\n",
    "            return X, y\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def prepare_data_for_training(self, X, y, test_size=0.2):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Normalize features\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val = self.scaler.transform(X_val)\n",
    "\n",
    "        return X_train, X_val, y_train, y_val\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        \"\"\"Train the Support Vector Regressor model and apply contrastive learning\"\"\"\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        # Train the contrastive learning model\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Labels are discrete for contrastive loss\n",
    "        \n",
    "        # Forward pass through the neural network\n",
    "        feature_output = self.neural_net(X_train_tensor)\n",
    "        \n",
    "        # Compute supervised contrastive loss\n",
    "        loss = self.criterion(feature_output, y_train_tensor)\n",
    "        print(f\"Contrastive learning loss: {loss.item()}\")\n",
    "\n",
    "    def save_model(self, model_save_path: str):\n",
    "        \"\"\"Save the trained model to a .pkl file using joblib\"\"\"\n",
    "        joblib.dump(self.model, model_save_path)\n",
    "        torch.save(self.neural_net.state_dict(), 'contrastive_model.pth')  # Save neural network model\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    def evaluate(self, X_val, y_val):\n",
    "        \"\"\"Evaluate the model performance on the validation set\"\"\"\n",
    "        predictions = self.model.predict(X_val)\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        mse = mean_squared_error(y_val, predictions)\n",
    "        r2 = r2_score(y_val, predictions)\n",
    "\n",
    "        return predictions, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive learning loss: 17855.833984375\n",
      "Model saved to svm_model.pkl\n",
      "Predictions: [ 0.01019536 -0.02527204  0.00274191  0.00208303  0.01178686 -0.00460234\n",
      "  0.05493183  0.00708125 -0.01250383  0.01796181 -0.00403191 -0.00844699\n",
      " -0.02870712 -0.00355368  0.00740673 -0.00132794 -0.00283145  0.00379296\n",
      " -0.00966893 -0.00488767 -0.01924176 -0.00651779  0.00597139  0.02262723\n",
      " -0.0146188  -0.01934731  0.0220529  -0.00506783 -0.00948003  0.0142224\n",
      "  0.01873508 -0.01186578 -0.00199399  0.00178427  0.01354984 -0.00896485\n",
      " -0.00798926 -0.00779012  0.00537058  0.00198901 -0.01336121 -0.02022063\n",
      " -0.00553066  0.01030152 -0.05896064  0.01057208 -0.01390117 -0.01589859\n",
      "  0.01008587 -0.01449242  0.02227062  0.01488097 -0.00262933  0.0031956\n",
      "  0.03342398  0.0182719   0.01219851 -0.01226846  0.00230924 -0.02311016\n",
      "  0.00717609 -0.00713348  0.01202055  0.07879168 -0.00707746 -0.00290128\n",
      "  0.00560496 -0.00360791 -0.015363    0.02042485  0.02325906 -0.01638165\n",
      " -0.02764853 -0.00847114  0.01068361  0.01671859 -0.00282981 -0.00367178\n",
      " -0.0028278  -0.00259404 -0.00337052 -0.00569889  0.01835597 -0.00462362\n",
      "  0.00049335 -0.00297699  0.0057292   0.00686286 -0.00891733  0.0732667\n",
      " -0.00185883  0.01600858  0.00492956  0.00284784 -0.00728307  0.00480244\n",
      "  0.00988022  0.0010294  -0.03025088 -0.01307021 -0.01388185  0.00206787\n",
      " -0.00012478 -0.00653364  0.00583984  0.02002219 -0.01687974  0.00093209\n",
      "  0.0058475  -0.03102837 -0.01210797 -0.02905301 -0.02466731  0.00127935\n",
      " -0.02176713 -0.00904014  0.02786066  0.00606247 -0.04111187 -0.00068653\n",
      "  0.0085651  -0.00282435  0.00397343  0.00343298  0.04832493 -0.01387641\n",
      " -0.01644164 -0.02728542 -0.01915985 -0.004441   -0.00760657  0.01280517\n",
      " -0.00501091 -0.01914363 -0.00132623  0.0099719  -0.00175373  0.00647443\n",
      " -0.00306126 -0.00867459  0.01488426  0.0206146   0.00171871 -0.00722804\n",
      " -0.01481159 -0.03778698 -0.01958225  0.00194376  0.00443516 -0.00555375\n",
      "  0.01322075 -0.0041121  -0.00168095  0.01330017 -0.01551467 -0.01570475\n",
      "  0.00494325 -0.00844178  0.00491162  0.00521332  0.00659278  0.01736334\n",
      "  0.01647667 -0.01084179 -0.0095636  -0.00325455  0.01532707  0.0005569\n",
      "  0.00097605  0.00361585  0.00135284  0.01320846 -0.00928818 -0.01856717\n",
      "  0.01344838  0.0300836   0.01307452 -0.00117926 -0.00751453  0.01707943\n",
      "  0.00358232  0.01093305  0.00890453  0.00237894  0.00898174 -0.00415263\n",
      "  0.00746772 -0.03204945 -0.00766043  0.00209466  0.00377145  0.00185494\n",
      " -0.01339162 -0.0040657   0.01773839  0.01187316 -0.00042964 -0.00094306\n",
      " -0.00302722  0.00465018 -0.03874538  0.00842889 -0.02964618 -0.00238046\n",
      " -0.00184713 -0.00946156  0.00296728 -0.00323403  0.01174325 -0.01508167\n",
      " -0.00106888  0.01637764  0.00343224  0.02708397 -0.0007983  -0.00920325\n",
      " -0.00318166  0.00762704 -0.00477992 -0.04797016  0.00114814  0.0062065\n",
      "  0.03284565 -0.00150771 -0.03164154  0.0318802  -0.00657535 -0.00237802\n",
      " -0.00301168 -0.0141699  -0.00030545  0.03339976 -0.00289376 -0.00038338\n",
      " -0.02111988 -0.01592918  0.01176936  0.00314126  0.00215211  0.00394524\n",
      " -0.00931783  0.01367415 -0.03259843  0.00378228 -0.0091873  -0.01377286\n",
      " -0.03177658  0.00901795  0.01067004 -0.02104809  0.00472441  0.00044686\n",
      "  0.02118255  0.00493702 -0.0004424   0.02808068  0.00408294 -0.00896491\n",
      " -0.00831391 -0.03756651 -0.00045325 -0.0055246   0.0113005   0.00815382\n",
      "  0.00487802  0.01506351 -0.0021195  -0.01093447  0.01100295 -0.00537534\n",
      "  0.01318679  0.00813377 -0.00570505 -0.00363762  0.00715775 -0.00308308\n",
      "  0.02579257 -0.01025799  0.00836421 -0.01331562 -0.01168008  0.00331805\n",
      " -0.00711059  0.00186715  0.00666018  0.00625902  0.00760597  0.05888342\n",
      "  0.03124983 -0.00595008 -0.00146416  0.00587217  0.01139125 -0.0028705\n",
      "  0.00220176  0.0341075   0.02553143 -0.00878763 -0.0021257   0.03185186\n",
      " -0.00350431  0.00566514 -0.02662541 -0.00048955 -0.01130007 -0.00763194\n",
      "  0.003085    0.00886572  0.0157843   0.00069987  0.00249598  0.01880555\n",
      " -0.00425199  0.00080259 -0.00748359 -0.01189594 -0.03071392 -0.00838923\n",
      "  0.0143504  -0.00518893 -0.00042247 -0.00412151  0.01611515  0.04280203\n",
      "  0.00446411 -0.01574706 -0.00688681  0.00107013 -0.01580623  0.01303109\n",
      "  0.01414536 -0.06201056 -0.03410286 -0.01854625 -0.00266641 -0.01014114\n",
      "  0.01268823 -0.02780662  0.02194309 -0.01985036 -0.01794506  0.00084926\n",
      " -0.00081537 -0.00868451 -0.00874373  0.00144682 -0.02259495  0.00755919\n",
      "  0.01566724  0.00583705  0.00374928 -0.0009608  -0.00826251 -0.01705706\n",
      "  0.0130012  -0.01958832  0.01632079  0.00598195  0.00555304  0.02261899\n",
      " -0.01626401 -0.00611555 -0.01426554  0.00163794  0.00642664  0.00827319\n",
      " -0.01423373 -0.007219   -0.00792665 -0.02267743 -0.01367983 -0.01974819\n",
      " -0.02751451  0.03408284 -0.00451992  0.01160111  0.00986547 -0.00126803\n",
      "  0.00741969  0.00092493  0.02380835 -0.01492904 -0.06924737  0.00655096\n",
      "  0.00358411 -0.00840696  0.0036697  -0.00274744  0.0106967  -0.00951589\n",
      " -0.00286246  0.0026988  -0.0245346  -0.0031571  -0.01246376  0.0043079\n",
      "  0.00552586  0.01089735  0.00865012  0.01140867  0.0335451  -0.0029695\n",
      " -0.0008914  -0.00104377 -0.00333845 -0.01028348 -0.00256731  0.00717317\n",
      " -0.00070081  0.02616341 -0.00265615 -0.02315795  0.00400174 -0.02647012\n",
      " -0.0006566  -0.00655194  0.0046678   0.01606351  0.00301952  0.00090231\n",
      " -0.02915117 -0.0296027   0.00516907 -0.02353007 -0.01959641 -0.0003067\n",
      " -0.01798202  0.01410212  0.01380404  0.00756419 -0.00044599 -0.01328426\n",
      "  0.01117625  0.00511367 -0.05723874 -0.00637107 -0.01063463 -0.01367468\n",
      " -0.01011619  0.02590309  0.00119584  0.00120476  0.0037475   0.01612688\n",
      "  0.01177075 -0.02496019  0.02764849  0.00505433 -0.00352755  0.03805088\n",
      "  0.00597148  0.0158428   0.00303912  0.02995123  0.00338808 -0.0184147\n",
      " -0.0078228   0.0022439   0.05249837 -0.00794125  0.01491564 -0.00497618\n",
      " -0.01497729 -0.00567365 -0.00287884 -0.03148798  0.01297157  0.01156969\n",
      " -0.00563347  0.01144593  0.00582693  0.00619752  0.01720046  0.00325186\n",
      "  0.0189403   0.01133067  0.00719536 -0.01280269  0.00312849  0.00028209\n",
      " -0.00525477 -0.00414515 -0.0028628  -0.01675368  0.01120236  0.02068315\n",
      " -0.00694008  0.00538901 -0.00887655  0.01372581  0.02052494 -0.00444201\n",
      "  0.01334658 -0.01487779 -0.00095274 -0.04669059 -0.01492035  0.02784053\n",
      "  0.0556479   0.01973014  0.00890477  0.01171931  0.02718366 -0.00849905\n",
      " -0.00564236  0.00433482  0.00707863 -0.01094369 -0.0011248  -0.00531001\n",
      "  0.00020162  0.02049859  0.00413968  0.00837111 -0.00324318  0.0250317\n",
      " -0.02894403  0.01163345 -0.0059371   0.01646731  0.00265608 -0.0026327\n",
      " -0.02758925  0.01345999 -0.00670492  0.007174   -0.04787533  0.0268378\n",
      " -0.01515111 -0.00011635 -0.01493253 -0.00624256  0.02479125 -0.00614743\n",
      " -0.00900879 -0.00057215 -0.0161199   0.0200109   0.02214242 -0.01658123\n",
      "  0.0159259   0.00621965 -0.00514806  0.00628521 -0.00627602  0.00073507\n",
      " -0.01885309  0.02103553 -0.00672608  0.01524416 -0.00350112 -0.0029777\n",
      "  0.01081218  0.0261077  -0.03122628 -0.00222845 -0.03643958 -0.07262259\n",
      " -0.02463914  0.01266218  0.0063333   0.00970196  0.00662634 -0.00347494\n",
      "  0.00557448 -0.01958769 -0.01317021 -0.00281485 -0.01426855  0.01454114\n",
      " -0.00510586  0.00761888 -0.00179359]\n",
      "Mean Squared Error: 0.00014275476632388421\n",
      "R-squared: 0.6299941343419084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize predictor\n",
    "    predictor = StockPredictor()\n",
    "\n",
    "    # Load and prepare data\n",
    "    X, y = predictor.load_data_from_csv(r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\features.csv\")\n",
    "    X_train, X_val, y_train, y_val = predictor.prepare_data_for_training(X, y)\n",
    "\n",
    "    # Train the model with contrastive learning\n",
    "    predictor.train_model(X_train, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    predictor.save_model(\"svm_model.pkl\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    predictions, mse, r2 = predictor.evaluate(X_val, y_val)\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
