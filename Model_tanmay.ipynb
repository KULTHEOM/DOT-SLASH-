{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Begin amigos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import (\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModel,\n",
    "# )\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# # from datetime import datetime, timedelta\n",
    "# import logging\n",
    "# import time\n",
    "# from typing import List, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NewsArticle:\n",
    "#     def __init__(self, title: str, content: str, date: str, source: str, url: str):\n",
    "#         self.title = title\n",
    "#         self.content = content\n",
    "#         self.date = date\n",
    "#         self.source = source\n",
    "#         self.url = url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FinancialNewsScraper:\n",
    "#     def __init__(self):\n",
    "#         self.headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "#         }\n",
    "#         self.sources = {\n",
    "#             'reuters': 'https://www.reuters.com/markets/companies',\n",
    "#             'marketwatch': 'https://www.marketwatch.com/markets',\n",
    "#             'investing': 'https://www.investing.com/news/stock-market-news'\n",
    "#         }\n",
    "\n",
    "#     def scrape_news(self, days_back: int = 7) -> List[NewsArticle]:\n",
    "#         articles = []\n",
    "#         for source, url in self.sources.items():\n",
    "#             try:\n",
    "#                 articles.extend(self._scrape_source(source, url, days_back))\n",
    "#                 time.sleep(2)  # Polite delay between sources\n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error scraping {source}: {str(e)}\")\n",
    "#         return articles\n",
    "\n",
    "#     def _scrape_source(self, source: str, url: str, days_back: int) -> List[NewsArticle]:\n",
    "#         articles = []\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=self.headers)\n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "#             if source == 'reuters':\n",
    "#                 articles.extend(self._parse_reuters(soup))\n",
    "#             elif source == 'marketwatch':\n",
    "#                 articles.extend(self._parse_marketwatch(soup))\n",
    "#             elif source == 'investing':\n",
    "#                 articles.extend(self._parse_investing(soup))\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in _scrape_source for {source}: {str(e)}\")\n",
    "            \n",
    "#         return articles\n",
    "\n",
    "#     def _parse_reuters(self, soup: BeautifulSoup) -> List[NewsArticle]:\n",
    "#         articles = []\n",
    "#         for article in soup.find_all('article'):\n",
    "#             try:\n",
    "#                 title = article.find('h3').text.strip()\n",
    "#                 link = article.find('a')['href']\n",
    "#                 article_content = self._get_article_content(f\"https://reuters.com{link}\")\n",
    "#                 date = datetime.now().strftime('%Y-%m-%d')  # Reuters articles usually have current date\n",
    "#                 articles.append(NewsArticle(title, article_content, date, 'Reuters', link))\n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error parsing Reuters article: {str(e)}\")\n",
    "#         return articles\n",
    "\n",
    "#     def _get_article_content(self, url: str) -> str:\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=self.headers)\n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#             paragraphs = soup.find_all('p')\n",
    "#             return ' '.join([p.text.strip() for p in paragraphs])\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error getting article content: {str(e)}\")\n",
    "#             return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FinancialDataset(Dataset):\n",
    "#     def __init__(self, data: np.ndarray, sequence_length: int, target_column: int):\n",
    "#         self.data = torch.FloatTensor(data)\n",
    "#         self.sequence_length = sequence_length\n",
    "#         self.target_column = target_column\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data) - self.sequence_length\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         x = self.data[idx:idx + self.sequence_length]\n",
    "#         y = self.data[idx + self.sequence_length][self.target_column]\n",
    "#         return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import logging\n",
    "# import yfinance as yf\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# class StockPredictor:\n",
    "#     def __init__(self,\n",
    "#                  time_series_model_name: str = \"huggingface/TimeSeriesTransformer\",\n",
    "#                  sentiment_model_name: str = \"ProsusAI/finbert\",\n",
    "#                  api_access_token: str = \"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\"):\n",
    "        \n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#         # Load time series model for regression\n",
    "#         self.time_series_model = AutoModel.from_pretrained(time_series_model_name, token=api_access_token).to(self.device)\n",
    "\n",
    "#         # Load sentiment analysis model\n",
    "#         self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name, token=api_access_token).to(self.device)\n",
    "#         self.sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name, token=api_access_token)\n",
    "        \n",
    "#         # Initialize scaler\n",
    "#         self.scaler = MinMaxScaler()\n",
    "\n",
    "#     def load_data_from_csv(self, file_path: str):\n",
    "#         \"\"\"Load stock data from a CSV file and split into features (X) and target (y)\"\"\"\n",
    "\n",
    "#         try:\n",
    "#             df = pd.read_csv(file_path)\n",
    "\n",
    "#             # Standardize column names (remove leading/trailing spaces)\n",
    "#             df.columns = df.columns.str.strip()\n",
    "\n",
    "#             # Expected columns\n",
    "#             expected_columns = [\n",
    "#                 \"date\", \"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\",\n",
    "#                 \"EMA\", \"Volume_Oscillator\", \"RSI\", \"%K\", \"%D\", \"+DI\", \"-DI\", \"ADX\", \"PVT\", \"Target\"\n",
    "#             ]\n",
    "\n",
    "#             missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "#             if missing_cols:\n",
    "#                 raise ValueError(f\"Missing columns in CSV file: {missing_cols}\")\n",
    "\n",
    "#             df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "#             df.dropna(subset=[\"date\"], inplace=True)\n",
    "#             df.set_index(\"date\", inplace=True)\n",
    "\n",
    "#             df[expected_columns[1:]] = df[expected_columns[1:]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "#             X = df.drop(columns=[\"Target\"]).values  # Features\n",
    "#             y = df[\"Target\"].values  # Target variable\n",
    "\n",
    "#             return X, y\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading CSV: {e}\")\n",
    "#             return None, None\n",
    "\n",
    "#     def prepare_data_for_training(self, X, y, test_size=0.2, batch_size=32):\n",
    "#         \"\"\"Prepare PyTorch data loaders\"\"\"\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "#         # Normalize features\n",
    "#         X_train = self.scaler.fit_transform(X_train)\n",
    "#         X_val = self.scaler.transform(X_val)\n",
    "\n",
    "#         # Convert to tensors\n",
    "#         X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "#         y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
    "#         X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)\n",
    "#         y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
    "\n",
    "#         train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#         val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         return train_loader, val_loader\n",
    "\n",
    "#     def train_model(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 10):\n",
    "#         \"\"\"Train the transformer model for time series regression\"\"\"\n",
    "#         optimizer = torch.optim.Adam(self.time_series_model.parameters(), lr=1e-4)\n",
    "#         criterion = torch.nn.MSELoss()\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             self.time_series_model.train()\n",
    "#             train_loss = 0\n",
    "\n",
    "#             for batch in train_loader:\n",
    "#                 batch_x, batch_y = batch\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 outputs = self.time_series_model(batch_x).logits  # Ensure model outputs logits\n",
    "#                 outputs = outputs.squeeze()\n",
    "\n",
    "#                 loss = criterion(outputs, batch_y)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 train_loss += loss.item()\n",
    "\n",
    "#             print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "#     def analyze_sentiment(self, text: str):\n",
    "#         \"\"\"Analyze sentiment of financial news\"\"\"\n",
    "#         inputs = self.sentiment_tokenizer(\n",
    "#             text,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=128\n",
    "#         ).to(self.device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.sentiment_model(**inputs)\n",
    "#             probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "#             prediction = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "#         return prediction.item(), probabilities.cpu().numpy()\n",
    "\n",
    "#     def predict_stock_price(self, X):\n",
    "#         \"\"\"Predict stock price using trained model\"\"\"\n",
    "#         self.time_series_model.eval()\n",
    "\n",
    "#         X_scaled = self.scaler.transform(X)\n",
    "#         X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             predictions = self.time_series_model(X_tensor).logits.squeeze().cpu().numpy()\n",
    "\n",
    "#         return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def main():\n",
    "#     logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    \n",
    "#     predictor_file_path = r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\features.csv\"\n",
    "#     predictor = StockPredictor()\n",
    "\n",
    "#     try:\n",
    "#         # Load stock data\n",
    "#         X, y = predictor.load_data_from_csv(predictor_file_path)\n",
    "\n",
    "#         if X is None or y is None:\n",
    "#             logging.error(\"Failed to load data.\")\n",
    "#             return\n",
    "\n",
    "#         # Prepare data loaders\n",
    "#         train_loader, val_loader = predictor.prepare_data_for_training(X, y)\n",
    "\n",
    "#         # Train model\n",
    "#         print(\"Training model...\")\n",
    "#         predictor.train_model(train_loader, val_loader)\n",
    "\n",
    "#         # Predict stock price\n",
    "#         X_test = X[-10:]  # Example: last 10 rows for prediction\n",
    "#         predictions = predictor.predict_stock_price(X_test)\n",
    "\n",
    "#         print(f\"\\nPredicted Stock Prices: {predictions}\")\n",
    "\n",
    "#         # Sentiment analysis example\n",
    "#         sentiment_result = predictor.analyze_sentiment(\"The stock market is experiencing a strong bullish trend.\")\n",
    "#         print(f\"\\nSentiment Analysis: {sentiment_result}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error in main execution: {str(e)}\")\n",
    "#         print(\"An error occurred. Check logs for details.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. volume</th>\n",
       "      <th>EMA</th>\n",
       "      <th>Volume_Oscillator</th>\n",
       "      <th>RSI</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>+DI</th>\n",
       "      <th>-DI</th>\n",
       "      <th>ADX</th>\n",
       "      <th>PVT</th>\n",
       "      <th>Target</th>\n",
       "      <th>Comprehensive_RRR</th>\n",
       "      <th>%Change</th>\n",
       "      <th>RRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-30 04:00:00</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.219129</td>\n",
       "      <td>43.196544</td>\n",
       "      <td>42.953016</td>\n",
       "      <td>40.810811</td>\n",
       "      <td>41.937054</td>\n",
       "      <td>17.661816</td>\n",
       "      <td>605.529682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-30 04:01:00</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>-81.818182</td>\n",
       "      <td>49.219129</td>\n",
       "      <td>43.196544</td>\n",
       "      <td>42.953016</td>\n",
       "      <td>40.810811</td>\n",
       "      <td>41.937054</td>\n",
       "      <td>17.661816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-12-30 04:02:00</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.366667</td>\n",
       "      <td>-35.714286</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-12-30 04:04:00</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.397778</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.092651</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-12-30 04:12:00</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.47</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2.421852</td>\n",
       "      <td>343.661972</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>52.631579</td>\n",
       "      <td>42.105263</td>\n",
       "      <td>70.370370</td>\n",
       "      <td>0.604846</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>2858</td>\n",
       "      <td>2025-01-28 16:00:00</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2974.0</td>\n",
       "      <td>1.977506</td>\n",
       "      <td>91.928753</td>\n",
       "      <td>60.754091</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>76.351228</td>\n",
       "      <td>57.776772</td>\n",
       "      <td>39.732850</td>\n",
       "      <td>20.583595</td>\n",
       "      <td>-1246.325133</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>2859</td>\n",
       "      <td>2025-01-28 16:13:00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.965004</td>\n",
       "      <td>-93.483290</td>\n",
       "      <td>54.233700</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>51.827782</td>\n",
       "      <td>39.845654</td>\n",
       "      <td>20.277913</td>\n",
       "      <td>-1249.325133</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>2860</td>\n",
       "      <td>2025-01-28 16:46:00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.976669</td>\n",
       "      <td>-99.217247</td>\n",
       "      <td>56.463955</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>54.084398</td>\n",
       "      <td>37.979094</td>\n",
       "      <td>21.205599</td>\n",
       "      <td>-1248.953999</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>2861</td>\n",
       "      <td>2025-01-28 18:28:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.964446</td>\n",
       "      <td>-99.018507</td>\n",
       "      <td>50.368374</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>48.447528</td>\n",
       "      <td>42.705984</td>\n",
       "      <td>20.654743</td>\n",
       "      <td>-1249.403999</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>2862</td>\n",
       "      <td>2025-01-28 19:51:00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.956297</td>\n",
       "      <td>-97.629786</td>\n",
       "      <td>53.841721</td>\n",
       "      <td>11.373708</td>\n",
       "      <td>50.457903</td>\n",
       "      <td>51.664817</td>\n",
       "      <td>38.901221</td>\n",
       "      <td>20.183610</td>\n",
       "      <td>-1249.403999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2863 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                 date  1. open  2. high  3. low  4. close  \\\n",
       "0              0  2024-12-30 04:00:00     2.36     2.36    2.36      2.36   \n",
       "1              1  2024-12-30 04:01:00     2.36     2.36    2.36      2.36   \n",
       "2              2  2024-12-30 04:02:00     2.38     2.38    2.38      2.38   \n",
       "3              3  2024-12-30 04:04:00     2.46     2.46    2.46      2.46   \n",
       "4              4  2024-12-30 04:12:00     2.38     2.47    2.38      2.47   \n",
       "...          ...                  ...      ...      ...     ...       ...   \n",
       "2858        2858  2025-01-28 16:00:00     1.97     2.00    1.97      2.00   \n",
       "2859        2859  2025-01-28 16:13:00     1.94     1.94    1.94      1.94   \n",
       "2860        2860  2025-01-28 16:46:00     1.99     2.00    1.99      2.00   \n",
       "2861        2861  2025-01-28 18:28:00     2.00     2.00    1.94      1.94   \n",
       "2862        2862  2025-01-28 19:51:00     1.94     1.94    1.94      1.94   \n",
       "\n",
       "      5. volume       EMA  Volume_Oscillator         RSI          %K  \\\n",
       "0          10.0  2.360000           0.000000   49.219129   43.196544   \n",
       "1           1.0  2.360000         -81.818182   49.219129   43.196544   \n",
       "2           3.0  2.366667         -35.714286  100.000000  100.000000   \n",
       "3           2.0  2.397778         -50.000000  100.000000  100.000000   \n",
       "4         126.0  2.421852         343.661972  100.000000  100.000000   \n",
       "...         ...       ...                ...         ...         ...   \n",
       "2858     2974.0  1.977506          91.928753   60.754091  100.000000   \n",
       "2859      100.0  1.965004         -93.483290   54.233700   40.000000   \n",
       "2860       12.0  1.976669         -99.217247   56.463955  100.000000   \n",
       "2861       15.0  1.964446         -99.018507   50.368374   40.000000   \n",
       "2862       36.0  1.956297         -97.629786   53.841721   11.373708   \n",
       "\n",
       "              %D         +DI        -DI         ADX          PVT    Target  \\\n",
       "0      42.953016   40.810811  41.937054   17.661816   605.529682  0.000000   \n",
       "1      42.953016   40.810811  41.937054   17.661816     0.000000  0.000000   \n",
       "2     100.000000  100.000000   0.000000  100.000000     0.025424  0.008475   \n",
       "3     100.000000  100.000000   0.000000  100.000000     0.092651  0.033613   \n",
       "4     100.000000   52.631579  42.105263   70.370370     0.604846  0.004065   \n",
       "...          ...         ...        ...         ...          ...       ...   \n",
       "2858   76.351228   57.776772  39.732850   20.583595 -1246.325133  0.015228   \n",
       "2859   70.000000   51.827782  39.845654   20.277913 -1249.325133 -0.030000   \n",
       "2860   80.000000   54.084398  37.979094   21.205599 -1248.953999  0.030928   \n",
       "2861   60.000000   48.447528  42.705984   20.654743 -1249.403999 -0.030000   \n",
       "2862   50.457903   51.664817  38.901221   20.183610 -1249.403999  0.000000   \n",
       "\n",
       "      Comprehensive_RRR   %Change       RRR  \n",
       "0              0.000968 -17.79661  1.242857  \n",
       "1              0.000968 -17.79661  1.242857  \n",
       "2              0.000968 -17.79661  1.242857  \n",
       "3              0.000968 -17.79661  1.242857  \n",
       "4              0.000968 -17.79661  1.242857  \n",
       "...                 ...       ...       ...  \n",
       "2858           0.005055 -17.79661  1.242857  \n",
       "2859           0.000000 -17.79661  1.242857  \n",
       "2860           0.001411 -17.79661  1.242857  \n",
       "2861           0.000000 -17.79661  1.242857  \n",
       "2862           0.000000 -17.79661  1.242857  \n",
       "\n",
       "[2863 rows x 20 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = pd.read_csv(r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\dataset_csv\\reliance.csv\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Unnamed: 0' in model.columns:\n",
    "    model.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>1. open</th>\n",
       "      <th>2. high</th>\n",
       "      <th>3. low</th>\n",
       "      <th>4. close</th>\n",
       "      <th>5. volume</th>\n",
       "      <th>EMA</th>\n",
       "      <th>Volume_Oscillator</th>\n",
       "      <th>RSI</th>\n",
       "      <th>%K</th>\n",
       "      <th>%D</th>\n",
       "      <th>+DI</th>\n",
       "      <th>-DI</th>\n",
       "      <th>ADX</th>\n",
       "      <th>PVT</th>\n",
       "      <th>Target</th>\n",
       "      <th>Comprehensive_RRR</th>\n",
       "      <th>%Change</th>\n",
       "      <th>RRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-30 04:00:00</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.219129</td>\n",
       "      <td>43.196544</td>\n",
       "      <td>42.953016</td>\n",
       "      <td>40.810811</td>\n",
       "      <td>41.937054</td>\n",
       "      <td>17.661816</td>\n",
       "      <td>605.529682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-30 04:01:00</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>-81.818182</td>\n",
       "      <td>49.219129</td>\n",
       "      <td>43.196544</td>\n",
       "      <td>42.953016</td>\n",
       "      <td>40.810811</td>\n",
       "      <td>41.937054</td>\n",
       "      <td>17.661816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-30 04:02:00</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.366667</td>\n",
       "      <td>-35.714286</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-30 04:04:00</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.397778</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.092651</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-30 04:12:00</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.47</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2.421852</td>\n",
       "      <td>343.661972</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>52.631579</td>\n",
       "      <td>42.105263</td>\n",
       "      <td>70.370370</td>\n",
       "      <td>0.604846</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>2025-01-28 16:00:00</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2974.0</td>\n",
       "      <td>1.977506</td>\n",
       "      <td>91.928753</td>\n",
       "      <td>60.754091</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>76.351228</td>\n",
       "      <td>57.776772</td>\n",
       "      <td>39.732850</td>\n",
       "      <td>20.583595</td>\n",
       "      <td>-1246.325133</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>2025-01-28 16:13:00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.965004</td>\n",
       "      <td>-93.483290</td>\n",
       "      <td>54.233700</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>51.827782</td>\n",
       "      <td>39.845654</td>\n",
       "      <td>20.277913</td>\n",
       "      <td>-1249.325133</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>2025-01-28 16:46:00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.976669</td>\n",
       "      <td>-99.217247</td>\n",
       "      <td>56.463955</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>54.084398</td>\n",
       "      <td>37.979094</td>\n",
       "      <td>21.205599</td>\n",
       "      <td>-1248.953999</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>2025-01-28 18:28:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.964446</td>\n",
       "      <td>-99.018507</td>\n",
       "      <td>50.368374</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>48.447528</td>\n",
       "      <td>42.705984</td>\n",
       "      <td>20.654743</td>\n",
       "      <td>-1249.403999</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>2025-01-28 19:51:00</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.94</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.956297</td>\n",
       "      <td>-97.629786</td>\n",
       "      <td>53.841721</td>\n",
       "      <td>11.373708</td>\n",
       "      <td>50.457903</td>\n",
       "      <td>51.664817</td>\n",
       "      <td>38.901221</td>\n",
       "      <td>20.183610</td>\n",
       "      <td>-1249.403999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.79661</td>\n",
       "      <td>1.242857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2863 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  1. open  2. high  3. low  4. close  5. volume  \\\n",
       "0     2024-12-30 04:00:00     2.36     2.36    2.36      2.36       10.0   \n",
       "1     2024-12-30 04:01:00     2.36     2.36    2.36      2.36        1.0   \n",
       "2     2024-12-30 04:02:00     2.38     2.38    2.38      2.38        3.0   \n",
       "3     2024-12-30 04:04:00     2.46     2.46    2.46      2.46        2.0   \n",
       "4     2024-12-30 04:12:00     2.38     2.47    2.38      2.47      126.0   \n",
       "...                   ...      ...      ...     ...       ...        ...   \n",
       "2858  2025-01-28 16:00:00     1.97     2.00    1.97      2.00     2974.0   \n",
       "2859  2025-01-28 16:13:00     1.94     1.94    1.94      1.94      100.0   \n",
       "2860  2025-01-28 16:46:00     1.99     2.00    1.99      2.00       12.0   \n",
       "2861  2025-01-28 18:28:00     2.00     2.00    1.94      1.94       15.0   \n",
       "2862  2025-01-28 19:51:00     1.94     1.94    1.94      1.94       36.0   \n",
       "\n",
       "           EMA  Volume_Oscillator         RSI          %K          %D  \\\n",
       "0     2.360000           0.000000   49.219129   43.196544   42.953016   \n",
       "1     2.360000         -81.818182   49.219129   43.196544   42.953016   \n",
       "2     2.366667         -35.714286  100.000000  100.000000  100.000000   \n",
       "3     2.397778         -50.000000  100.000000  100.000000  100.000000   \n",
       "4     2.421852         343.661972  100.000000  100.000000  100.000000   \n",
       "...        ...                ...         ...         ...         ...   \n",
       "2858  1.977506          91.928753   60.754091  100.000000   76.351228   \n",
       "2859  1.965004         -93.483290   54.233700   40.000000   70.000000   \n",
       "2860  1.976669         -99.217247   56.463955  100.000000   80.000000   \n",
       "2861  1.964446         -99.018507   50.368374   40.000000   60.000000   \n",
       "2862  1.956297         -97.629786   53.841721   11.373708   50.457903   \n",
       "\n",
       "             +DI        -DI         ADX          PVT    Target  \\\n",
       "0      40.810811  41.937054   17.661816   605.529682  0.000000   \n",
       "1      40.810811  41.937054   17.661816     0.000000  0.000000   \n",
       "2     100.000000   0.000000  100.000000     0.025424  0.008475   \n",
       "3     100.000000   0.000000  100.000000     0.092651  0.033613   \n",
       "4      52.631579  42.105263   70.370370     0.604846  0.004065   \n",
       "...          ...        ...         ...          ...       ...   \n",
       "2858   57.776772  39.732850   20.583595 -1246.325133  0.015228   \n",
       "2859   51.827782  39.845654   20.277913 -1249.325133 -0.030000   \n",
       "2860   54.084398  37.979094   21.205599 -1248.953999  0.030928   \n",
       "2861   48.447528  42.705984   20.654743 -1249.403999 -0.030000   \n",
       "2862   51.664817  38.901221   20.183610 -1249.403999  0.000000   \n",
       "\n",
       "      Comprehensive_RRR   %Change       RRR  \n",
       "0              0.000968 -17.79661  1.242857  \n",
       "1              0.000968 -17.79661  1.242857  \n",
       "2              0.000968 -17.79661  1.242857  \n",
       "3              0.000968 -17.79661  1.242857  \n",
       "4              0.000968 -17.79661  1.242857  \n",
       "...                 ...       ...       ...  \n",
       "2858           0.005055 -17.79661  1.242857  \n",
       "2859           0.000000 -17.79661  1.242857  \n",
       "2860           0.001411 -17.79661  1.242857  \n",
       "2861           0.000000 -17.79661  1.242857  \n",
       "2862           0.000000 -17.79661  1.242857  \n",
       "\n",
       "[2863 rows x 19 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_csv(r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\dataset_csv\\reliance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "import joblib  # For saving scaler\n",
    "\n",
    "class StockPredictor:\n",
    "    def __init__(self, input_size=14, num_classes=2):  # Adjust input size to 14 after dropping 5 features\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "        # Complex Neural Network Model with Dropout and Batch Normalization\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),  # Increase initial layer size\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),  # Batch Normalization to stabilize learning\n",
    "            nn.Dropout(p=0.2),  # Dropout to prevent overfitting\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),  # Batch Normalization\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),  # Batch Normalization\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Linear(32, num_classes),  # Output layer for 2 classes\n",
    "            nn.Softmax(dim=1)  # Softmax for multi-class classification\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def load_data_from_csv(self, file_path: str):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, index_col=0)\n",
    "            df.columns = df.columns.str.strip()  # Clean column names\n",
    "\n",
    "            required_columns = {\n",
    "                \"date\", \"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\",\n",
    "                \"EMA\", \"Volume_Oscillator\", \"RSI\", \"%K\", \"%D\", \"+DI\", \"-DI\", \"ADX\", \"PVT\",\n",
    "                \"Target\", \"Comprehensive_RRR\", \"%Change\", \"RRR\"\n",
    "            }\n",
    "\n",
    "            if not required_columns.issubset(df.columns):\n",
    "                raise ValueError(f\"Missing required columns in {file_path}\")\n",
    "\n",
    "            df = df.apply(pd.to_numeric, errors=\"coerce\")  # Convert to numeric\n",
    "\n",
    "            # Drop unnecessary features including 'date'\n",
    "            X = df.drop(columns=[\"Target\", \"Comprehensive_RRR\", \"%Change\", \"RRR\", \"date\"]).values\n",
    "            y = df[[\"Target\", \"Comprehensive_RRR\"]].astype(int).values  # Combined target\n",
    "\n",
    "            print(f\"Loaded {file_path}: Features {X.shape}, Targets {y.shape}\")  # Debugging\n",
    "            return X, y\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV {file_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def prepare_data_for_training(self, X, y, test_size=0.2):\n",
    "        \"\"\"Split and scale data, ensuring correct tensor types.\"\"\"\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val = self.scaler.transform(X_val)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(X_val, dtype=torch.float32),\n",
    "            torch.tensor(y_train[:, 0], dtype=torch.long),  # Extract first target column\n",
    "            torch.tensor(y_val[:, 0], dtype=torch.long),  \n",
    "            torch.tensor(y_train[:, 1], dtype=torch.long),  # Extract second target column\n",
    "            torch.tensor(y_val[:, 1], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def train_model(self, X_train, y_target_train, y_rrr_train, epochs=100):\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(X_train)  # Single forward pass\n",
    "\n",
    "            loss_target = self.criterion(output, y_target_train)\n",
    "            loss_rrr = self.criterion(output, y_rrr_train)\n",
    "\n",
    "            loss = loss_target + loss_rrr  # Combined loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Save both model and scaler after training\n",
    "        self.save_model(\"model.pt\")\n",
    "        self.save_scaler(\"scaler.pkl\")\n",
    "\n",
    "    def save_model(self, model_path: str):\n",
    "        # Save the entire model (including architecture, weights, etc.)\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        print(f\"Model saved at {model_path}\")\n",
    "\n",
    "    def save_scaler(self, scaler_path: str):\n",
    "        # Save the scaler object to a pickle file\n",
    "        joblib.dump(self.scaler, scaler_path)\n",
    "        print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "    def evaluate(self, X_val, y_target_val, y_rrr_val):\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = self.model(X_val)\n",
    "            pred_target = torch.argmax(pred, dim=1)\n",
    "            pred_rrr = torch.argmax(pred, dim=1)\n",
    "\n",
    "        # Accuracy metrics\n",
    "        acc_target = accuracy_score(y_target_val.numpy(), pred_target.numpy())\n",
    "        acc_rrr = accuracy_score(y_rrr_val.numpy(), pred_rrr.numpy())\n",
    "        \n",
    "        # Regression metrics (for R2 and MSE)\n",
    "        r2_target = r2_score(y_target_val.numpy(), pred_target.numpy())\n",
    "        r2_rrr = r2_score(y_rrr_val.numpy(), pred_rrr.numpy())\n",
    "\n",
    "        mse_target = mean_squared_error(y_target_val.numpy(), pred_target.numpy())\n",
    "        mse_rrr = mean_squared_error(y_rrr_val.numpy(), pred_rrr.numpy())\n",
    "\n",
    "        return acc_target, acc_rrr, r2_target, r2_rrr, mse_target, mse_rrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing infosys.csv...\n",
      "Loaded dataset_csv\\infosys.csv: Features (8819, 14), Targets (8819, 2)\n",
      "Epoch [10/100], Loss: 1.3993\n",
      "Epoch [20/100], Loss: 1.2627\n",
      "Epoch [30/100], Loss: 1.1661\n",
      "Epoch [40/100], Loss: 1.0862\n",
      "Epoch [50/100], Loss: 1.0262\n",
      "Epoch [60/100], Loss: 0.9813\n",
      "Epoch [70/100], Loss: 0.9520\n",
      "Epoch [80/100], Loss: 0.9276\n",
      "Epoch [90/100], Loss: 0.9097\n",
      "Epoch [100/100], Loss: 0.8982\n",
      "Model saved at model.pt\n",
      "Scaler saved at scaler.pkl\n",
      "Model saved at nn_model_infosys.pth\n",
      "Scaler saved at scaler_infosys.pkl\n",
      "infosys - Target Accuracy: 0.89, Comprehensive_RRR Accuracy: 0.89\n",
      "infosys - R2 Target: 0.00, R2 Comprehensive_RRR: 0.00\n",
      "infosys - MSE Target: 0.11, MSE Comprehensive_RRR: 0.11\n",
      "Processing reliance.csv...\n",
      "Loaded dataset_csv\\reliance.csv: Features (2863, 14), Targets (2863, 2)\n",
      "Epoch [10/100], Loss: 0.8767\n",
      "Epoch [20/100], Loss: 0.8615\n",
      "Epoch [30/100], Loss: 0.8466\n",
      "Epoch [40/100], Loss: 0.8348\n",
      "Epoch [50/100], Loss: 0.8241\n",
      "Epoch [60/100], Loss: 0.8160\n",
      "Epoch [70/100], Loss: 0.8090\n",
      "Epoch [80/100], Loss: 0.8046\n",
      "Epoch [90/100], Loss: 0.7974\n",
      "Epoch [100/100], Loss: 0.7827\n",
      "Model saved at model.pt\n",
      "Scaler saved at scaler.pkl\n",
      "Model saved at nn_model_reliance.pth\n",
      "Scaler saved at scaler_reliance.pkl\n",
      "reliance - Target Accuracy: 0.93, Comprehensive_RRR Accuracy: 0.93\n",
      "reliance - R2 Target: 0.00, R2 Comprehensive_RRR: 0.00\n",
      "reliance - MSE Target: 0.07, MSE Comprehensive_RRR: 0.07\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "def main():\n",
    "    dataset_folder = \"dataset_csv\"\n",
    "    \n",
    "    if not os.path.exists(dataset_folder):\n",
    "        print(f\"Dataset folder '{dataset_folder}' not found.\")\n",
    "        return\n",
    "    \n",
    "    predictor = StockPredictor()\n",
    "    \n",
    "    for file in os.listdir(dataset_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            company_name = os.path.splitext(file)[0]\n",
    "            model_path = f\"nn_model_{company_name}.pth\"\n",
    "            scaler_path = f\"scaler_{company_name}.pkl\"  # Path for the scaler\n",
    "\n",
    "            print(f\"Processing {file}...\")\n",
    "            X, y = predictor.load_data_from_csv(os.path.join(dataset_folder, file))\n",
    "            \n",
    "            if X is not None and y is not None:\n",
    "                X_train, X_val, y_target_train, y_target_val, y_rrr_train, y_rrr_val = (\n",
    "                    predictor.prepare_data_for_training(X, y)\n",
    "                )\n",
    "                \n",
    "                predictor.train_model(X_train, y_target_train, y_rrr_train)\n",
    "                \n",
    "                # Save both the model and optimizer state\n",
    "                predictor.save_model(model_path)\n",
    "                \n",
    "                # Save the scaler object as a pickle file\n",
    "                with open(scaler_path, 'wb') as f:\n",
    "                    pickle.dump(predictor.scaler, f)\n",
    "                print(f\"Scaler saved at {scaler_path}\")\n",
    "                \n",
    "                # Unpack all 6 metrics returned from evaluate\n",
    "                acc_target, acc_rrr, r2_target, r2_rrr, mse_target, mse_rrr = predictor.evaluate(X_val, y_target_val, y_rrr_val)\n",
    "                \n",
    "                # Print relevant metrics\n",
    "                print(f\"{company_name} - Target Accuracy: {acc_target:.2f}, Comprehensive_RRR Accuracy: {acc_rrr:.2f}\")\n",
    "                print(f\"{company_name} - R2 Target: {r2_target:.2f}, R2 Comprehensive_RRR: {r2_rrr:.2f}\")\n",
    "                print(f\"{company_name} - MSE Target: {mse_target:.2f}, MSE Comprehensive_RRR: {mse_rrr:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on the pt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "class StockPredictor:\n",
    "    def __init__(self, input_size=14, num_classes=2):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(32, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        # Load the trained model from the specified path\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_scaler(self, scaler_path):\n",
    "        # Load the scaler from the specified path\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        X = df.drop(columns=[\"Target\", \"Comprehensive_RRR\", \"%Change\", \"RRR\", \"date\"]).values\n",
    "        X = self.scaler.transform(X)  # Use the loaded scaler for preprocessing\n",
    "        return torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            output = self.model(X)\n",
    "            pred_target = torch.argmax(output, dim=1)\n",
    "            return pred_target.numpy()\n",
    "\n",
    "def main():\n",
    "    model_path = r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\nn_model_reliance.pth\"\n",
    "    scaler_path = r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\scaler_reliance.pkl\"  # Add path for scaler\n",
    "    csv_path = r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\dataset_testing\\reliance_testing.csv\"\n",
    "\n",
    "    predictor = StockPredictor()\n",
    "    predictor.load_model(model_path)  # Load the model\n",
    "    predictor.load_scaler(scaler_path)  # Load the scaler\n",
    "\n",
    "    df = pd.read_csv(csv_path, index_col=0)\n",
    "    X = predictor.preprocess_data(df)\n",
    "\n",
    "    predictions = predictor.predict(X)\n",
    "    print(\"Predictions:\", predictions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic analysis of the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: semantic_news_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def fetch_news(ticker, api_key):\n",
    "    search_url = f\"https://newsapi.org/v2/everything?q={ticker}&apiKey={api_key}\"\n",
    "    response = requests.get(search_url)\n",
    "    if response.status_code == 200:\n",
    "        articles = response.json().get(\"articles\", [])\n",
    "        return [(article['title'], article['description'], article['url']) for article in articles]\n",
    "    else:\n",
    "        print(\"Error fetching news:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "def analyze_sentiment(news_list):\n",
    "    sentiment_model = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "    results = []\n",
    "    for title, description, url in news_list:\n",
    "        text = title if title else description  # Use title first, fallback to description\n",
    "        sentiment = sentiment_model(text)[0]\n",
    "        label = sentiment[\"label\"].lower()\n",
    "        results.append((title, description, url, label))\n",
    "    return results\n",
    "\n",
    "def save_to_csv(sentiment_data, filename=\"semantic_news_analysis.csv\"):\n",
    "    df = pd.DataFrame(sentiment_data, columns=[\"Title\", \"Description\", \"URL\", \"Sentiment\"])\n",
    "    df = df[df['Title'].str.contains(\"RELIANCE INDUSTRIES\", case=False, na=False) | df['Description'].str.contains(\"RELIANCE\", case=False, na=False)]\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"CSV saved: {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TICKER = \"RELIANCE INDUSTRIES\"\n",
    "    API_KEY = \"ee237929c6fc4dfd868f2a6bf68e4494\"  # Replace with your API key\n",
    "    \n",
    "    news_data = fetch_news(TICKER, API_KEY)\n",
    "    if news_data:\n",
    "        sentiment_results = analyze_sentiment(news_data)\n",
    "        save_to_csv(sentiment_results)\n",
    "    else:\n",
    "        print(\"No news data fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: sentiment_news_analysis_reliance.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import pipeline\n",
    "\n",
    "def fetch_news(ticker, start_date, end_date, api_key):\n",
    "    search_url = f\"https://serpapi.com/search?q={ticker}&api_key={api_key}&tbm=nws\"\n",
    "    \n",
    "    news_data = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        # Format date to match the SERP API\n",
    "        formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "        response = requests.get(f\"{search_url}&tbs=qdr:d&as_of={formatted_date}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            news_results = data.get('news_results', [])\n",
    "            for article in news_results:\n",
    "                news_data.append({\n",
    "                    'title': article['title'],\n",
    "                    'description': article['snippet'],\n",
    "                    'url': article['link'],\n",
    "                    'timestamp': article.get('date', formatted_date)  # Capture the date\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Error fetching news for {current_date}: {response.status_code}\")\n",
    "        \n",
    "        # Increment to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def analyze_sentiment(news_list):\n",
    "    sentiment_model = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "    results = []\n",
    "    for article in news_list:\n",
    "        text = article['title'] if article['title'] else article['description']  # Use title first, fallback to description\n",
    "        sentiment = sentiment_model(text)[0]\n",
    "        label = sentiment[\"label\"].lower()\n",
    "        results.append({\n",
    "            'Title': article['title'],\n",
    "            'Description': article['description'],\n",
    "            'URL': article['url'],\n",
    "            'Sentiment': label,\n",
    "            'Timestamp': article['timestamp']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def save_to_csv(sentiment_data, filename=\"sentiment_news_analysis_reliance.csv\"):\n",
    "    df = pd.DataFrame(sentiment_data, columns=[\"Title\", \"Description\", \"URL\", \"Sentiment\", \"Timestamp\"])\n",
    "    \n",
    "    # Filter articles for 'RELIANCE INDUSTRIES' from Title or Description\n",
    "    df = df[df['Title'].str.contains(\"RELIANCE INDUSTRIES\", case=False, na=False) | \n",
    "            df['Description'].str.contains(\"RELIANCE\", case=False, na=False)]\n",
    "    \n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"CSV saved: {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TICKER = \"RELIANCE INDUSTRIES\"\n",
    "    API_KEY = \"891402407a8a59b9fcbe0456b271884db6a6ba57bb9acfdc1d65cf6e0a003dfc\"  # Your SERP API key\n",
    "    START_DATE = datetime(2025, 1, 24)\n",
    "    END_DATE = datetime(2025, 1, 31)\n",
    "    \n",
    "    # Fetch news data between the specified dates\n",
    "    news_data = fetch_news(TICKER, START_DATE, END_DATE, API_KEY)\n",
    "    \n",
    "    if news_data:\n",
    "        sentiment_results = analyze_sentiment(news_data)\n",
    "        save_to_csv(sentiment_results)\n",
    "    else:\n",
    "        print(\"No news data fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anant Ambani drives efforts towards a safer, m...</td>\n",
       "      <td>Reliance Industries Limited has launched 'Teer...</td>\n",
       "      <td>https://m.economictimes.com/news/company/corpo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reliance Industries Limited</td>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>https://m.facebook.com/100067675925751/photos/...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>38 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>As millions of devotees converge in Prayagraj,...</td>\n",
       "      <td>https://www.indiatoday.in/amp/impact-feature/s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reliance Launches ‘Teerth Yatri Seva’ At Maha ...</td>\n",
       "      <td>With millions of devotees gathering in Prayagr...</td>\n",
       "      <td>https://www.msn.com/en-in/news/India/reliance-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>India's richest person wants to build the worl...</td>\n",
       "      <td>Mukesh Ambani's Reliance Group, one of India's...</td>\n",
       "      <td>https://www.techradar.com/pro/indias-richest-p...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anant Ambani drives efforts towards a safer, m...</td>\n",
       "      <td>Reliance Industries Limited has launched 'Teer...</td>\n",
       "      <td>https://m.economictimes.com/news/company/corpo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Reliance Industries Limited</td>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>https://m.facebook.com/100067675925751/photos/...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>38 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>As millions of devotees converge in Prayagraj,...</td>\n",
       "      <td>https://www.indiatoday.in/amp/impact-feature/s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Reliance Launches ‘Teerth Yatri Seva’ At Maha ...</td>\n",
       "      <td>With millions of devotees gathering in Prayagr...</td>\n",
       "      <td>https://www.msn.com/en-in/news/India/reliance-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>India's richest person wants to build the worl...</td>\n",
       "      <td>Mukesh Ambani's Reliance Group, one of India's...</td>\n",
       "      <td>https://www.techradar.com/pro/indias-richest-p...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Anant Ambani drives efforts towards a safer, m...</td>\n",
       "      <td>Reliance Industries Limited has launched 'Teer...</td>\n",
       "      <td>https://m.economictimes.com/news/company/corpo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Reliance Industries Limited</td>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>https://m.facebook.com/100067675925751/photos/...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>38 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>As millions of devotees converge in Prayagraj,...</td>\n",
       "      <td>https://www.indiatoday.in/amp/impact-feature/s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Reliance Launches ‘Teerth Yatri Seva’ At Maha ...</td>\n",
       "      <td>With millions of devotees gathering in Prayagr...</td>\n",
       "      <td>https://www.msn.com/en-in/news/India/reliance-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>India's richest person wants to build the worl...</td>\n",
       "      <td>Mukesh Ambani's Reliance Group, one of India's...</td>\n",
       "      <td>https://www.techradar.com/pro/indias-richest-p...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Anant Ambani drives efforts towards a safer, m...</td>\n",
       "      <td>Reliance Industries Limited has launched 'Teer...</td>\n",
       "      <td>https://m.economictimes.com/news/company/corpo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Reliance Industries Limited</td>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>https://m.facebook.com/100067675925751/photos/...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>38 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>As millions of devotees converge in Prayagraj,...</td>\n",
       "      <td>https://www.indiatoday.in/amp/impact-feature/s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Reliance Launches ‘Teerth Yatri Seva’ At Maha ...</td>\n",
       "      <td>With millions of devotees gathering in Prayagr...</td>\n",
       "      <td>https://www.msn.com/en-in/news/India/reliance-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>India's richest person wants to build the worl...</td>\n",
       "      <td>Mukesh Ambani's Reliance Group, one of India's...</td>\n",
       "      <td>https://www.techradar.com/pro/indias-richest-p...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Anant Ambani drives efforts towards a safer, m...</td>\n",
       "      <td>Reliance Industries Limited has launched 'Teer...</td>\n",
       "      <td>https://m.economictimes.com/news/company/corpo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Reliance Industries Limited</td>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>https://m.facebook.com/100067675925751/photos/...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>38 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>As millions of devotees converge in Prayagraj,...</td>\n",
       "      <td>https://www.indiatoday.in/amp/impact-feature/s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Reliance Launches ‘Teerth Yatri Seva’ At Maha ...</td>\n",
       "      <td>With millions of devotees gathering in Prayagr...</td>\n",
       "      <td>https://www.msn.com/en-in/news/India/reliance-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>India's richest person wants to build the worl...</td>\n",
       "      <td>Mukesh Ambani's Reliance Group, one of India's...</td>\n",
       "      <td>https://www.techradar.com/pro/indias-richest-p...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Anant Ambani drives efforts towards a safer, m...</td>\n",
       "      <td>Reliance Industries Limited has launched 'Teer...</td>\n",
       "      <td>https://m.economictimes.com/news/company/corpo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Reliance Industries Limited</td>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>https://m.facebook.com/100067675925751/photos/...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>38 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>As millions of devotees converge in Prayagraj,...</td>\n",
       "      <td>https://www.indiatoday.in/amp/impact-feature/s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Reliance Launches ‘Teerth Yatri Seva’ At Maha ...</td>\n",
       "      <td>With millions of devotees gathering in Prayagr...</td>\n",
       "      <td>https://www.msn.com/en-in/news/India/reliance-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>India's richest person wants to build the worl...</td>\n",
       "      <td>Mukesh Ambani's Reliance Group, one of India's...</td>\n",
       "      <td>https://www.techradar.com/pro/indias-richest-p...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Anant Ambani drives efforts towards a safer, m...</td>\n",
       "      <td>Reliance Industries Limited has launched 'Teer...</td>\n",
       "      <td>https://m.economictimes.com/news/company/corpo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Reliance Industries Limited</td>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>https://m.facebook.com/100067675925751/photos/...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>38 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>As millions of devotees converge in Prayagraj,...</td>\n",
       "      <td>https://www.indiatoday.in/amp/impact-feature/s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Reliance Launches ‘Teerth Yatri Seva’ At Maha ...</td>\n",
       "      <td>With millions of devotees gathering in Prayagr...</td>\n",
       "      <td>https://www.msn.com/en-in/news/India/reliance-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>India's richest person wants to build the worl...</td>\n",
       "      <td>Mukesh Ambani's Reliance Group, one of India's...</td>\n",
       "      <td>https://www.techradar.com/pro/indias-richest-p...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Anant Ambani drives efforts towards a safer, m...</td>\n",
       "      <td>Reliance Industries Limited has launched 'Teer...</td>\n",
       "      <td>https://m.economictimes.com/news/company/corpo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Reliance Industries Limited</td>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>https://m.facebook.com/100067675925751/photos/...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>38 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>As millions of devotees converge in Prayagraj,...</td>\n",
       "      <td>https://www.indiatoday.in/amp/impact-feature/s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Reliance Launches ‘Teerth Yatri Seva’ At Maha ...</td>\n",
       "      <td>With millions of devotees gathering in Prayagr...</td>\n",
       "      <td>https://www.msn.com/en-in/news/India/reliance-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>India's richest person wants to build the worl...</td>\n",
       "      <td>Mukesh Ambani's Reliance Group, one of India's...</td>\n",
       "      <td>https://www.techradar.com/pro/indias-richest-p...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   Anant Ambani drives efforts towards a safer, m...   \n",
       "1                         Reliance Industries Limited   \n",
       "2   Reliance Industries chairman Mukesh Ambani to ...   \n",
       "3   Reliance serves millions at Maha Kumbh to faci...   \n",
       "4   Reliance serves millions at Maha Kumbh to faci...   \n",
       "5   Reliance Launches ‘Teerth Yatri Seva’ At Maha ...   \n",
       "6   India's richest person wants to build the worl...   \n",
       "7   Anant Ambani drives efforts towards a safer, m...   \n",
       "8                         Reliance Industries Limited   \n",
       "9   Reliance Industries chairman Mukesh Ambani to ...   \n",
       "10  Reliance serves millions at Maha Kumbh to faci...   \n",
       "11  Reliance serves millions at Maha Kumbh to faci...   \n",
       "12  Reliance Launches ‘Teerth Yatri Seva’ At Maha ...   \n",
       "13  India's richest person wants to build the worl...   \n",
       "14  Anant Ambani drives efforts towards a safer, m...   \n",
       "15                        Reliance Industries Limited   \n",
       "16  Reliance Industries chairman Mukesh Ambani to ...   \n",
       "17  Reliance serves millions at Maha Kumbh to faci...   \n",
       "18  Reliance serves millions at Maha Kumbh to faci...   \n",
       "19  Reliance Launches ‘Teerth Yatri Seva’ At Maha ...   \n",
       "20  India's richest person wants to build the worl...   \n",
       "21  Anant Ambani drives efforts towards a safer, m...   \n",
       "22                        Reliance Industries Limited   \n",
       "23  Reliance Industries chairman Mukesh Ambani to ...   \n",
       "24  Reliance serves millions at Maha Kumbh to faci...   \n",
       "25  Reliance serves millions at Maha Kumbh to faci...   \n",
       "26  Reliance Launches ‘Teerth Yatri Seva’ At Maha ...   \n",
       "27  India's richest person wants to build the worl...   \n",
       "28  Anant Ambani drives efforts towards a safer, m...   \n",
       "29                        Reliance Industries Limited   \n",
       "30  Reliance Industries chairman Mukesh Ambani to ...   \n",
       "31  Reliance serves millions at Maha Kumbh to faci...   \n",
       "32  Reliance serves millions at Maha Kumbh to faci...   \n",
       "33  Reliance Launches ‘Teerth Yatri Seva’ At Maha ...   \n",
       "34  India's richest person wants to build the worl...   \n",
       "35  Anant Ambani drives efforts towards a safer, m...   \n",
       "36                        Reliance Industries Limited   \n",
       "37  Reliance Industries chairman Mukesh Ambani to ...   \n",
       "38  Reliance serves millions at Maha Kumbh to faci...   \n",
       "39  Reliance serves millions at Maha Kumbh to faci...   \n",
       "40  Reliance Launches ‘Teerth Yatri Seva’ At Maha ...   \n",
       "41  India's richest person wants to build the worl...   \n",
       "42  Anant Ambani drives efforts towards a safer, m...   \n",
       "43                        Reliance Industries Limited   \n",
       "44  Reliance Industries chairman Mukesh Ambani to ...   \n",
       "45  Reliance serves millions at Maha Kumbh to faci...   \n",
       "46  Reliance serves millions at Maha Kumbh to faci...   \n",
       "47  Reliance Launches ‘Teerth Yatri Seva’ At Maha ...   \n",
       "48  India's richest person wants to build the worl...   \n",
       "49  Anant Ambani drives efforts towards a safer, m...   \n",
       "50                        Reliance Industries Limited   \n",
       "51  Reliance Industries chairman Mukesh Ambani to ...   \n",
       "52  Reliance serves millions at Maha Kumbh to faci...   \n",
       "53  Reliance serves millions at Maha Kumbh to faci...   \n",
       "54  Reliance Launches ‘Teerth Yatri Seva’ At Maha ...   \n",
       "55  India's richest person wants to build the worl...   \n",
       "\n",
       "                                          Description  \\\n",
       "0   Reliance Industries Limited has launched 'Teer...   \n",
       "1   Reliance serves millions at Maha Kumbh to faci...   \n",
       "2   TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "3   Guided by its 'We Care' philosophy, Reliance i...   \n",
       "4   As millions of devotees converge in Prayagraj,...   \n",
       "5   With millions of devotees gathering in Prayagr...   \n",
       "6   Mukesh Ambani's Reliance Group, one of India's...   \n",
       "7   Reliance Industries Limited has launched 'Teer...   \n",
       "8   Reliance serves millions at Maha Kumbh to faci...   \n",
       "9   TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "10  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "11  As millions of devotees converge in Prayagraj,...   \n",
       "12  With millions of devotees gathering in Prayagr...   \n",
       "13  Mukesh Ambani's Reliance Group, one of India's...   \n",
       "14  Reliance Industries Limited has launched 'Teer...   \n",
       "15  Reliance serves millions at Maha Kumbh to faci...   \n",
       "16  TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "17  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "18  As millions of devotees converge in Prayagraj,...   \n",
       "19  With millions of devotees gathering in Prayagr...   \n",
       "20  Mukesh Ambani's Reliance Group, one of India's...   \n",
       "21  Reliance Industries Limited has launched 'Teer...   \n",
       "22  Reliance serves millions at Maha Kumbh to faci...   \n",
       "23  TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "24  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "25  As millions of devotees converge in Prayagraj,...   \n",
       "26  With millions of devotees gathering in Prayagr...   \n",
       "27  Mukesh Ambani's Reliance Group, one of India's...   \n",
       "28  Reliance Industries Limited has launched 'Teer...   \n",
       "29  Reliance serves millions at Maha Kumbh to faci...   \n",
       "30  TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "31  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "32  As millions of devotees converge in Prayagraj,...   \n",
       "33  With millions of devotees gathering in Prayagr...   \n",
       "34  Mukesh Ambani's Reliance Group, one of India's...   \n",
       "35  Reliance Industries Limited has launched 'Teer...   \n",
       "36  Reliance serves millions at Maha Kumbh to faci...   \n",
       "37  TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "38  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "39  As millions of devotees converge in Prayagraj,...   \n",
       "40  With millions of devotees gathering in Prayagr...   \n",
       "41  Mukesh Ambani's Reliance Group, one of India's...   \n",
       "42  Reliance Industries Limited has launched 'Teer...   \n",
       "43  Reliance serves millions at Maha Kumbh to faci...   \n",
       "44  TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "45  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "46  As millions of devotees converge in Prayagraj,...   \n",
       "47  With millions of devotees gathering in Prayagr...   \n",
       "48  Mukesh Ambani's Reliance Group, one of India's...   \n",
       "49  Reliance Industries Limited has launched 'Teer...   \n",
       "50  Reliance serves millions at Maha Kumbh to faci...   \n",
       "51  TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "52  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "53  As millions of devotees converge in Prayagraj,...   \n",
       "54  With millions of devotees gathering in Prayagr...   \n",
       "55  Mukesh Ambani's Reliance Group, one of India's...   \n",
       "\n",
       "                                                  URL Sentiment  \\\n",
       "0   https://m.economictimes.com/news/company/corpo...  positive   \n",
       "1   https://m.facebook.com/100067675925751/photos/...   neutral   \n",
       "2   https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "3   https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "4   https://www.indiatoday.in/amp/impact-feature/s...   neutral   \n",
       "5   https://www.msn.com/en-in/news/India/reliance-...  positive   \n",
       "6   https://www.techradar.com/pro/indias-richest-p...   neutral   \n",
       "7   https://m.economictimes.com/news/company/corpo...  positive   \n",
       "8   https://m.facebook.com/100067675925751/photos/...   neutral   \n",
       "9   https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "10  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "11  https://www.indiatoday.in/amp/impact-feature/s...   neutral   \n",
       "12  https://www.msn.com/en-in/news/India/reliance-...  positive   \n",
       "13  https://www.techradar.com/pro/indias-richest-p...   neutral   \n",
       "14  https://m.economictimes.com/news/company/corpo...  positive   \n",
       "15  https://m.facebook.com/100067675925751/photos/...   neutral   \n",
       "16  https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "17  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "18  https://www.indiatoday.in/amp/impact-feature/s...   neutral   \n",
       "19  https://www.msn.com/en-in/news/India/reliance-...  positive   \n",
       "20  https://www.techradar.com/pro/indias-richest-p...   neutral   \n",
       "21  https://m.economictimes.com/news/company/corpo...  positive   \n",
       "22  https://m.facebook.com/100067675925751/photos/...   neutral   \n",
       "23  https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "24  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "25  https://www.indiatoday.in/amp/impact-feature/s...   neutral   \n",
       "26  https://www.msn.com/en-in/news/India/reliance-...  positive   \n",
       "27  https://www.techradar.com/pro/indias-richest-p...   neutral   \n",
       "28  https://m.economictimes.com/news/company/corpo...  positive   \n",
       "29  https://m.facebook.com/100067675925751/photos/...   neutral   \n",
       "30  https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "31  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "32  https://www.indiatoday.in/amp/impact-feature/s...   neutral   \n",
       "33  https://www.msn.com/en-in/news/India/reliance-...  positive   \n",
       "34  https://www.techradar.com/pro/indias-richest-p...   neutral   \n",
       "35  https://m.economictimes.com/news/company/corpo...  positive   \n",
       "36  https://m.facebook.com/100067675925751/photos/...   neutral   \n",
       "37  https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "38  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "39  https://www.indiatoday.in/amp/impact-feature/s...   neutral   \n",
       "40  https://www.msn.com/en-in/news/India/reliance-...  positive   \n",
       "41  https://www.techradar.com/pro/indias-richest-p...   neutral   \n",
       "42  https://m.economictimes.com/news/company/corpo...  positive   \n",
       "43  https://m.facebook.com/100067675925751/photos/...   neutral   \n",
       "44  https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "45  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "46  https://www.indiatoday.in/amp/impact-feature/s...   neutral   \n",
       "47  https://www.msn.com/en-in/news/India/reliance-...  positive   \n",
       "48  https://www.techradar.com/pro/indias-richest-p...   neutral   \n",
       "49  https://m.economictimes.com/news/company/corpo...  positive   \n",
       "50  https://m.facebook.com/100067675925751/photos/...   neutral   \n",
       "51  https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "52  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "53  https://www.indiatoday.in/amp/impact-feature/s...   neutral   \n",
       "54  https://www.msn.com/en-in/news/India/reliance-...  positive   \n",
       "55  https://www.techradar.com/pro/indias-richest-p...   neutral   \n",
       "\n",
       "         Timestamp  \n",
       "0      5 hours ago  \n",
       "1      9 hours ago  \n",
       "2     17 hours ago  \n",
       "3   38 minutes ago  \n",
       "4      4 hours ago  \n",
       "5      4 hours ago  \n",
       "6      2 hours ago  \n",
       "7      5 hours ago  \n",
       "8      9 hours ago  \n",
       "9     17 hours ago  \n",
       "10  38 minutes ago  \n",
       "11     4 hours ago  \n",
       "12     4 hours ago  \n",
       "13     2 hours ago  \n",
       "14     5 hours ago  \n",
       "15     9 hours ago  \n",
       "16    17 hours ago  \n",
       "17  38 minutes ago  \n",
       "18     4 hours ago  \n",
       "19     4 hours ago  \n",
       "20     2 hours ago  \n",
       "21     5 hours ago  \n",
       "22     9 hours ago  \n",
       "23    17 hours ago  \n",
       "24  38 minutes ago  \n",
       "25     4 hours ago  \n",
       "26     4 hours ago  \n",
       "27     2 hours ago  \n",
       "28     5 hours ago  \n",
       "29     9 hours ago  \n",
       "30    17 hours ago  \n",
       "31  38 minutes ago  \n",
       "32     4 hours ago  \n",
       "33     4 hours ago  \n",
       "34     2 hours ago  \n",
       "35     5 hours ago  \n",
       "36     9 hours ago  \n",
       "37    17 hours ago  \n",
       "38  38 minutes ago  \n",
       "39     4 hours ago  \n",
       "40     4 hours ago  \n",
       "41     2 hours ago  \n",
       "42     5 hours ago  \n",
       "43     9 hours ago  \n",
       "44    17 hours ago  \n",
       "45  38 minutes ago  \n",
       "46     4 hours ago  \n",
       "47     4 hours ago  \n",
       "48     2 hours ago  \n",
       "49     5 hours ago  \n",
       "50     9 hours ago  \n",
       "51    17 hours ago  \n",
       "52  38 minutes ago  \n",
       "53     4 hours ago  \n",
       "54     4 hours ago  \n",
       "55     2 hours ago  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\sentiment_news_analysis_reliance.csv\")\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay\\AppData\\Local\\Temp\\ipykernel_18352\\3297718310.py:70: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with filled missing minutes saved: filled_sentiment_news_analysis_minutes_reliance.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import parsedatetime\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to parse relative datetime strings like \"5 hours ago\"\n",
    "def parse_relative_time(time_str):\n",
    "    cal = parsedatetime.Calendar()\n",
    "    time_struct, _ = cal.parse(time_str)\n",
    "    return datetime(*time_struct[:6])\n",
    "\n",
    "def fetch_news(ticker, start_date, end_date, api_key):\n",
    "    search_url = f\"https://serpapi.com/search?q={ticker}&api_key={api_key}&tbm=nws\"\n",
    "    \n",
    "    news_data = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        # Format date to match the SERP API\n",
    "        formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "        response = requests.get(f\"{search_url}&tbs=qdr:d&as_of={formatted_date}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            news_results = data.get('news_results', [])\n",
    "            for article in news_results:\n",
    "                news_data.append({\n",
    "                    'title': article['title'],\n",
    "                    'description': article['snippet'],\n",
    "                    'url': article['link'],\n",
    "                    'timestamp': article.get('date', formatted_date)  # Capture the date\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Error fetching news for {current_date}: {response.status_code}\")\n",
    "        \n",
    "        # Increment to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def analyze_sentiment(news_list):\n",
    "    sentiment_model = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "    results = []\n",
    "    for article in news_list:\n",
    "        text = article['title'] if article['title'] else article['description']  # Use title first, fallback to description\n",
    "        sentiment = sentiment_model(text)[0]\n",
    "        label = sentiment[\"label\"].lower()\n",
    "        results.append({\n",
    "            'Title': article['title'],\n",
    "            'Description': article['description'],\n",
    "            'URL': article['url'],\n",
    "            'Sentiment': label,\n",
    "            'Timestamp': article['timestamp']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def fill_missing_minutes(csv_file, output_file=\"filled_sentiment_news_analysis_minutes_reliance.csv\"):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Parse the relative timestamp strings into actual datetime values\n",
    "    df['Timestamp'] = df['Timestamp'].apply(parse_relative_time)\n",
    "\n",
    "    # Sort the data by Timestamp\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "\n",
    "    # Create a complete minute range from the minimum to the maximum timestamp\n",
    "    min_time = df['Timestamp'].min().replace(second=0, microsecond=0)\n",
    "    max_time = df['Timestamp'].max().replace(second=0, microsecond=0)\n",
    "    minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n",
    "\n",
    "    # Initialize an empty list to store new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through the minute range and fill missing minutes with the closest data\n",
    "    for minute in minute_range:\n",
    "        # Get the closest data entry before or at this minute\n",
    "        closest_row = df[df['Timestamp'] <= minute].iloc[-1:]\n",
    "\n",
    "        if closest_row.empty:\n",
    "            continue  # Skip if no data is available before this minute\n",
    "\n",
    "        # Duplicate the data for the current minute\n",
    "        new_row = closest_row.copy()\n",
    "        new_row['Timestamp'] = minute\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    # Concatenate the original dataframe with the new rows\n",
    "    df_filled = pd.concat([df, pd.concat(new_rows)]).sort_values(by='Timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Save the updated data to a new CSV file\n",
    "    df_filled.to_csv(output_file, index=False)\n",
    "    print(f\"CSV with filled missing minutes saved: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "fill_missing_minutes(\"sentiment_news_analysis_reliance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-01 09:33:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-01 09:33:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-01 09:33:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-01 09:33:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reliance Industries chairman Mukesh Ambani to ...</td>\n",
       "      <td>TECH NEWS : Mukesh Ambani advises students to ...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-01 09:33:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-02 01:55:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-02 01:55:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-02 01:55:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-02 01:55:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>Reliance serves millions at Maha Kumbh to faci...</td>\n",
       "      <td>Guided by its 'We Care' philosophy, Reliance i...</td>\n",
       "      <td>https://www.firstpost.com/india/reliance-serve...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2025-02-02 01:55:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1038 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0     Reliance Industries chairman Mukesh Ambani to ...   \n",
       "1     Reliance Industries chairman Mukesh Ambani to ...   \n",
       "2     Reliance Industries chairman Mukesh Ambani to ...   \n",
       "3     Reliance Industries chairman Mukesh Ambani to ...   \n",
       "4     Reliance Industries chairman Mukesh Ambani to ...   \n",
       "...                                                 ...   \n",
       "1033  Reliance serves millions at Maha Kumbh to faci...   \n",
       "1034  Reliance serves millions at Maha Kumbh to faci...   \n",
       "1035  Reliance serves millions at Maha Kumbh to faci...   \n",
       "1036  Reliance serves millions at Maha Kumbh to faci...   \n",
       "1037  Reliance serves millions at Maha Kumbh to faci...   \n",
       "\n",
       "                                            Description  \\\n",
       "0     TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "1     TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "2     TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "3     TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "4     TECH NEWS : Mukesh Ambani advises students to ...   \n",
       "...                                                 ...   \n",
       "1033  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "1034  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "1035  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "1036  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "1037  Guided by its 'We Care' philosophy, Reliance i...   \n",
       "\n",
       "                                                    URL Sentiment  \\\n",
       "0     https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "1     https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "2     https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "3     https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "4     https://timesofindia.indiatimes.com/technology...   neutral   \n",
       "...                                                 ...       ...   \n",
       "1033  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "1034  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "1035  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "1036  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "1037  https://www.firstpost.com/india/reliance-serve...   neutral   \n",
       "\n",
       "                Timestamp  \n",
       "0     2025-02-01 09:33:18  \n",
       "1     2025-02-01 09:33:18  \n",
       "2     2025-02-01 09:33:18  \n",
       "3     2025-02-01 09:33:18  \n",
       "4     2025-02-01 09:33:18  \n",
       "...                   ...  \n",
       "1033  2025-02-02 01:55:18  \n",
       "1034  2025-02-02 01:55:18  \n",
       "1035  2025-02-02 01:55:18  \n",
       "1036  2025-02-02 01:55:18  \n",
       "1037  2025-02-02 01:55:18  \n",
       "\n",
       "[1038 rows x 5 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_news = pd.read_csv(r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\filled_sentiment_news_analysis_minutes_reliance.csv\")\n",
    "updated_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay\\AppData\\Local\\Temp\\ipykernel_18352\\1713005790.py:17: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with filled missing minutes and duplicates dropped saved: filled_sentiment_news_analysis_minutes_no_duplicates_reliance.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def fill_missing_minutes_and_drop_duplicates(csv_file, output_file=\"filled_sentiment_news_analysis_minutes_no_duplicates_reliance.csv\"):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Parse the relative timestamp strings into actual datetime values\n",
    "    df['Timestamp'] = df['Timestamp'].apply(parse_relative_time)\n",
    "\n",
    "    # Sort the data by Timestamp\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "\n",
    "    # Create a complete minute range from the minimum to the maximum timestamp\n",
    "    min_time = df['Timestamp'].min().replace(second=0, microsecond=0)\n",
    "    max_time = df['Timestamp'].max().replace(second=0, microsecond=0)\n",
    "    minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n",
    "\n",
    "    # Initialize an empty list to store new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through the minute range and fill missing minutes with the closest data\n",
    "    for minute in minute_range:\n",
    "        # Get the closest data entry before or at this minute\n",
    "        closest_row = df[df['Timestamp'] <= minute].iloc[-1:]\n",
    "\n",
    "        if closest_row.empty:\n",
    "            continue  # Skip if no data is available before this minute\n",
    "\n",
    "        # Duplicate the data for the current minute\n",
    "        new_row = closest_row.copy()\n",
    "        new_row['Timestamp'] = minute\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    # Concatenate the original dataframe with the new rows\n",
    "    df_filled = pd.concat([df, pd.concat(new_rows)]).sort_values(by='Timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Adjust the timestamp to always show 31 January 2025 while preserving the time details\n",
    "    fixed_date = datetime(2025, 1, 31)  # Set the date to 31 January 2025\n",
    "    df_filled['Timestamp'] = df_filled['Timestamp'].apply(lambda x: x.replace(year=fixed_date.year, month=fixed_date.month, day=fixed_date.day))\n",
    "\n",
    "    # Drop duplicate rows based on Timestamp and Sentiment\n",
    "    df_filled = df_filled.drop_duplicates(subset=['Timestamp', 'Sentiment'], keep='first')\n",
    "\n",
    "    # Save the updated data to a new CSV file\n",
    "    df_filled.to_csv(output_file, index=False)\n",
    "    print(f\"CSV with filled missing minutes and duplicates dropped saved: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "fill_missing_minutes_and_drop_duplicates(\"filled_sentiment_news_analysis_minutes_reliance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: sentiment_news_analysis_infosys.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import pipeline\n",
    "\n",
    "def fetch_news(ticker, start_date, end_date, api_key):\n",
    "    search_url = f\"https://serpapi.com/search?q={ticker}&api_key={api_key}&tbm=nws\"\n",
    "    \n",
    "    news_data = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        # Format date to match the SERP API\n",
    "        formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "        response = requests.get(f\"{search_url}&tbs=qdr:d&as_of={formatted_date}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            news_results = data.get('news_results', [])\n",
    "            for article in news_results:\n",
    "                news_data.append({\n",
    "                    'title': article['title'],\n",
    "                    'description': article['snippet'],\n",
    "                    'url': article['link'],\n",
    "                    'timestamp': article.get('date', formatted_date)  # Capture the date\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Error fetching news for {current_date}: {response.status_code}\")\n",
    "        \n",
    "        # Increment to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def analyze_sentiment(news_list):\n",
    "    sentiment_model = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "    results = []\n",
    "    for article in news_list:\n",
    "        text = article['title'] if article['title'] else article['description']  # Use title first, fallback to description\n",
    "        sentiment = sentiment_model(text)[0]\n",
    "        label = sentiment[\"label\"].lower()\n",
    "        results.append({\n",
    "            'Title': article['title'],\n",
    "            'Description': article['description'],\n",
    "            'URL': article['url'],\n",
    "            'Sentiment': label,\n",
    "            'Timestamp': article['timestamp']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def save_to_csv(sentiment_data, filename=\"sentiment_news_analysis_infosys.csv\"):\n",
    "    df = pd.DataFrame(sentiment_data, columns=[\"Title\", \"Description\", \"URL\", \"Sentiment\", \"Timestamp\"])\n",
    "    \n",
    "    # Filter articles for 'INFOSYS INDUSTRIES' from Title or Description\n",
    "    df = df[df['Title'].str.contains(\"INFOSYS\", case=False, na=False) | \n",
    "            df['Description'].str.contains(\"INFOSYS\", case=False, na=False)]\n",
    "    \n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"CSV saved: {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TICKER = \"INFOSYS \"\n",
    "    API_KEY = \"891402407a8a59b9fcbe0456b271884db6a6ba57bb9acfdc1d65cf6e0a003dfc\"  # Your SERP API key\n",
    "    START_DATE = datetime(2025, 1, 24)\n",
    "    END_DATE = datetime(2025, 1, 31)\n",
    "    \n",
    "    # Fetch news data between the specified dates\n",
    "    news_data = fetch_news(TICKER, START_DATE, END_DATE, API_KEY)\n",
    "    \n",
    "    if news_data:\n",
    "        sentiment_results = analyze_sentiment(news_data)\n",
    "        save_to_csv(sentiment_results)\n",
    "    else:\n",
    "        print(\"No news data fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay\\AppData\\Local\\Temp\\ipykernel_18352\\2861729338.py:70: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with filled missing minutes saved: filled_sentiment_news_analysis_minutes_infosys.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import parsedatetime\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to parse relative datetime strings like \"5 hours ago\"\n",
    "def parse_relative_time(time_str):\n",
    "    cal = parsedatetime.Calendar()\n",
    "    time_struct, _ = cal.parse(time_str)\n",
    "    return datetime(*time_struct[:6])\n",
    "\n",
    "def fetch_news(ticker, start_date, end_date, api_key):\n",
    "    search_url = f\"https://serpapi.com/search?q={ticker}&api_key={api_key}&tbm=nws\"\n",
    "    \n",
    "    news_data = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        # Format date to match the SERP API\n",
    "        formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "        response = requests.get(f\"{search_url}&tbs=qdr:d&as_of={formatted_date}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            news_results = data.get('news_results', [])\n",
    "            for article in news_results:\n",
    "                news_data.append({\n",
    "                    'title': article['title'],\n",
    "                    'description': article['snippet'],\n",
    "                    'url': article['link'],\n",
    "                    'timestamp': article.get('date', formatted_date)  # Capture the date\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Error fetching news for {current_date}: {response.status_code}\")\n",
    "        \n",
    "        # Increment to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def analyze_sentiment(news_list):\n",
    "    sentiment_model = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "    results = []\n",
    "    for article in news_list:\n",
    "        text = article['title'] if article['title'] else article['description']  # Use title first, fallback to description\n",
    "        sentiment = sentiment_model(text)[0]\n",
    "        label = sentiment[\"label\"].lower()\n",
    "        results.append({\n",
    "            'Title': article['title'],\n",
    "            'Description': article['description'],\n",
    "            'URL': article['url'],\n",
    "            'Sentiment': label,\n",
    "            'Timestamp': article['timestamp']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def fill_missing_minutes(csv_file, output_file=\"filled_sentiment_news_analysis_minutes_infosys.csv\"):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Parse the relative timestamp strings into actual datetime values\n",
    "    df['Timestamp'] = df['Timestamp'].apply(parse_relative_time)\n",
    "\n",
    "    # Sort the data by Timestamp\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "\n",
    "    # Create a complete minute range from the minimum to the maximum timestamp\n",
    "    min_time = df['Timestamp'].min().replace(second=0, microsecond=0)\n",
    "    max_time = df['Timestamp'].max().replace(second=0, microsecond=0)\n",
    "    minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n",
    "\n",
    "    # Initialize an empty list to store new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through the minute range and fill missing minutes with the closest data\n",
    "    for minute in minute_range:\n",
    "        # Get the closest data entry before or at this minute\n",
    "        closest_row = df[df['Timestamp'] <= minute].iloc[-1:]\n",
    "\n",
    "        if closest_row.empty:\n",
    "            continue  # Skip if no data is available before this minute\n",
    "\n",
    "        # Duplicate the data for the current minute\n",
    "        new_row = closest_row.copy()\n",
    "        new_row['Timestamp'] = minute\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    # Concatenate the original dataframe with the new rows\n",
    "    df_filled = pd.concat([df, pd.concat(new_rows)]).sort_values(by='Timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Save the updated data to a new CSV file\n",
    "    df_filled.to_csv(output_file, index=False)\n",
    "    print(f\"CSV with filled missing minutes saved: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "fill_missing_minutes(\"sentiment_news_analysis_infosys.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay\\AppData\\Local\\Temp\\ipykernel_18352\\4017151503.py:17: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with filled missing minutes and duplicates dropped saved: filled_sentiment_news_analysis_minutes_no_duplicates_infosys.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def fill_missing_minutes_and_drop_duplicates(csv_file, output_file=\"filled_sentiment_news_analysis_minutes_no_duplicates_infosys.csv\"):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Parse the relative timestamp strings into actual datetime values\n",
    "    df['Timestamp'] = df['Timestamp'].apply(parse_relative_time)\n",
    "\n",
    "    # Sort the data by Timestamp\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "\n",
    "    # Create a complete minute range from the minimum to the maximum timestamp\n",
    "    min_time = df['Timestamp'].min().replace(second=0, microsecond=0)\n",
    "    max_time = df['Timestamp'].max().replace(second=0, microsecond=0)\n",
    "    minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n",
    "\n",
    "    # Initialize an empty list to store new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through the minute range and fill missing minutes with the closest data\n",
    "    for minute in minute_range:\n",
    "        # Get the closest data entry before or at this minute\n",
    "        closest_row = df[df['Timestamp'] <= minute].iloc[-1:]\n",
    "\n",
    "        if closest_row.empty:\n",
    "            continue  # Skip if no data is available before this minute\n",
    "\n",
    "        # Duplicate the data for the current minute\n",
    "        new_row = closest_row.copy()\n",
    "        new_row['Timestamp'] = minute\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    # Concatenate the original dataframe with the new rows\n",
    "    df_filled = pd.concat([df, pd.concat(new_rows)]).sort_values(by='Timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Adjust the timestamp to always show 31 January 2025 while preserving the time details\n",
    "    fixed_date = datetime(2025, 1, 31)  # Set the date to 31 January 2025\n",
    "    df_filled['Timestamp'] = df_filled['Timestamp'].apply(lambda x: x.replace(year=fixed_date.year, month=fixed_date.month, day=fixed_date.day))\n",
    "\n",
    "    # Drop duplicate rows based on Timestamp and Sentiment\n",
    "    df_filled = df_filled.drop_duplicates(subset=['Timestamp', 'Sentiment'], keep='first')\n",
    "\n",
    "    # Save the updated data to a new CSV file\n",
    "    df_filled.to_csv(output_file, index=False)\n",
    "    print(f\"CSV with filled missing minutes and duplicates dropped saved: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "fill_missing_minutes_and_drop_duplicates(\"filled_sentiment_news_analysis_minutes_infosys.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved: sentiment_news_analysis_AAPL.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import pipeline\n",
    "\n",
    "def fetch_news(ticker, start_date, end_date, api_key):\n",
    "    search_url = f\"https://serpapi.com/search?q={ticker}&api_key={api_key}&tbm=nws\"\n",
    "    \n",
    "    news_data = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        # Format date to match the SERP API\n",
    "        formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "        response = requests.get(f\"{search_url}&tbs=qdr:d&as_of={formatted_date}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            news_results = data.get('news_results', [])\n",
    "            for article in news_results:\n",
    "                news_data.append({\n",
    "                    'title': article['title'],\n",
    "                    'description': article['snippet'],\n",
    "                    'url': article['link'],\n",
    "                    'timestamp': article.get('date', formatted_date)  # Capture the date\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Error fetching news for {current_date}: {response.status_code}\")\n",
    "        \n",
    "        # Increment to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def analyze_sentiment(news_list):\n",
    "    sentiment_model = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "    results = []\n",
    "    for article in news_list:\n",
    "        text = article['title'] if article['title'] else article['description']  # Use title first, fallback to description\n",
    "        sentiment = sentiment_model(text)[0]\n",
    "        label = sentiment[\"label\"].lower()\n",
    "        results.append({\n",
    "            'Title': article['title'],\n",
    "            'Description': article['description'],\n",
    "            'URL': article['url'],\n",
    "            'Sentiment': label,\n",
    "            'Timestamp': article['timestamp']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def save_to_csv(sentiment_data, filename=\"sentiment_news_analysis_AAPL.csv\"):\n",
    "    df = pd.DataFrame(sentiment_data, columns=[\"Title\", \"Description\", \"URL\", \"Sentiment\", \"Timestamp\"])\n",
    "    \n",
    "    # Filter articles for 'INFOSYS INDUSTRIES' from Title or Description\n",
    "    df = df[df['Title'].str.contains(\"Apple\", case=False, na=False) | \n",
    "            df['Description'].str.contains(\"Apple\", case=False, na=False)]\n",
    "    \n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"CSV saved: {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TICKER = \"Apple\"\n",
    "    API_KEY = \"891402407a8a59b9fcbe0456b271884db6a6ba57bb9acfdc1d65cf6e0a003dfc\"  # Your SERP API key\n",
    "    START_DATE = datetime(2025, 1, 24)\n",
    "    END_DATE = datetime(2025, 1, 31)\n",
    "    \n",
    "    # Fetch news data between the specified dates\n",
    "    news_data = fetch_news(TICKER, START_DATE, END_DATE, API_KEY)\n",
    "    \n",
    "    if news_data:\n",
    "        sentiment_results = analyze_sentiment(news_data)\n",
    "        save_to_csv(sentiment_results)\n",
    "    else:\n",
    "        print(\"No news data fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay\\AppData\\Local\\Temp\\ipykernel_8816\\3068592535.py:70: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with filled missing minutes saved: filled_sentiment_news_analysis_minutes_AAPL.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import parsedatetime\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to parse relative datetime strings like \"5 hours ago\"\n",
    "def parse_relative_time(time_str):\n",
    "    cal = parsedatetime.Calendar()\n",
    "    time_struct, _ = cal.parse(time_str)\n",
    "    return datetime(*time_struct[:6])\n",
    "\n",
    "def fetch_news(ticker, start_date, end_date, api_key):\n",
    "    search_url = f\"https://serpapi.com/search?q={ticker}&api_key={api_key}&tbm=nws\"\n",
    "    \n",
    "    news_data = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        # Format date to match the SERP API\n",
    "        formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "        response = requests.get(f\"{search_url}&tbs=qdr:d&as_of={formatted_date}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            news_results = data.get('news_results', [])\n",
    "            for article in news_results:\n",
    "                news_data.append({\n",
    "                    'title': article['title'],\n",
    "                    'description': article['snippet'],\n",
    "                    'url': article['link'],\n",
    "                    'timestamp': article.get('date', formatted_date)  # Capture the date\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Error fetching news for {current_date}: {response.status_code}\")\n",
    "        \n",
    "        # Increment to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def analyze_sentiment(news_list):\n",
    "    sentiment_model = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "    results = []\n",
    "    for article in news_list:\n",
    "        text = article['title'] if article['title'] else article['description']  # Use title first, fallback to description\n",
    "        sentiment = sentiment_model(text)[0]\n",
    "        label = sentiment[\"label\"].lower()\n",
    "        results.append({\n",
    "            'Title': article['title'],\n",
    "            'Description': article['description'],\n",
    "            'URL': article['url'],\n",
    "            'Sentiment': label,\n",
    "            'Timestamp': article['timestamp']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def fill_missing_minutes(csv_file, output_file=\"filled_sentiment_news_analysis_minutes_AAPL.csv\"):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Parse the relative timestamp strings into actual datetime values\n",
    "    df['Timestamp'] = df['Timestamp'].apply(parse_relative_time)\n",
    "\n",
    "    # Sort the data by Timestamp\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "\n",
    "    # Create a complete minute range from the minimum to the maximum timestamp\n",
    "    min_time = df['Timestamp'].min().replace(second=0, microsecond=0)\n",
    "    max_time = df['Timestamp'].max().replace(second=0, microsecond=0)\n",
    "    minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n",
    "\n",
    "    # Initialize an empty list to store new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through the minute range and fill missing minutes with the closest data\n",
    "    for minute in minute_range:\n",
    "        # Get the closest data entry before or at this minute\n",
    "        closest_row = df[df['Timestamp'] <= minute].iloc[-1:]\n",
    "\n",
    "        if closest_row.empty:\n",
    "            continue  # Skip if no data is available before this minute\n",
    "\n",
    "        # Duplicate the data for the current minute\n",
    "        new_row = closest_row.copy()\n",
    "        new_row['Timestamp'] = minute\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    # Concatenate the original dataframe with the new rows\n",
    "    df_filled = pd.concat([df, pd.concat(new_rows)]).sort_values(by='Timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Save the updated data to a new CSV file\n",
    "    df_filled.to_csv(output_file, index=False)\n",
    "    print(f\"CSV with filled missing minutes saved: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "fill_missing_minutes(\"sentiment_news_analysis_AAPL.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay\\AppData\\Local\\Temp\\ipykernel_8816\\1603883639.py:17: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with filled missing minutes and duplicates dropped saved: filled_sentiment_news_analysis_minutes_no_duplicates_AAPL.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def fill_missing_minutes_and_drop_duplicates(csv_file, output_file=\"filled_sentiment_news_analysis_minutes_no_duplicates_AAPL.csv\"):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Parse the relative timestamp strings into actual datetime values\n",
    "    df['Timestamp'] = df['Timestamp'].apply(parse_relative_time)\n",
    "\n",
    "    # Sort the data by Timestamp\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "\n",
    "    # Create a complete minute range from the minimum to the maximum timestamp\n",
    "    min_time = df['Timestamp'].min().replace(second=0, microsecond=0)\n",
    "    max_time = df['Timestamp'].max().replace(second=0, microsecond=0)\n",
    "    minute_range = pd.date_range(min_time, max_time, freq='T')  # 'T' is for minute frequency\n",
    "\n",
    "    # Initialize an empty list to store new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through the minute range and fill missing minutes with the closest data\n",
    "    for minute in minute_range:\n",
    "        # Get the closest data entry before or at this minute\n",
    "        closest_row = df[df['Timestamp'] <= minute].iloc[-1:]\n",
    "\n",
    "        if closest_row.empty:\n",
    "            continue  # Skip if no data is available before this minute\n",
    "\n",
    "        # Duplicate the data for the current minute\n",
    "        new_row = closest_row.copy()\n",
    "        new_row['Timestamp'] = minute\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    # Concatenate the original dataframe with the new rows\n",
    "    df_filled = pd.concat([df, pd.concat(new_rows)]).sort_values(by='Timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Adjust the timestamp to always show 31 January 2025 while preserving the time details\n",
    "    fixed_date = datetime(2025, 1, 31)  # Set the date to 31 January 2025\n",
    "    df_filled['Timestamp'] = df_filled['Timestamp'].apply(lambda x: x.replace(year=fixed_date.year, month=fixed_date.month, day=fixed_date.day))\n",
    "\n",
    "    # Drop duplicate rows based on Timestamp and Sentiment\n",
    "    df_filled = df_filled.drop_duplicates(subset=['Timestamp', 'Sentiment'], keep='first')\n",
    "\n",
    "    # Save the updated data to a new CSV file\n",
    "    df_filled.to_csv(output_file, index=False)\n",
    "    print(f\"CSV with filled missing minutes and duplicates dropped saved: {output_file}\")\n",
    "\n",
    "# Usage\n",
    "fill_missing_minutes_and_drop_duplicates(\"filled_sentiment_news_analysis_minutes_AAPL.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'stock_transactions.csv' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# List of stock symbols\n",
    "stock_symbols = ['AAPL', 'GOOGL', 'AMZN', 'MSFT', 'TSLA', 'FB', 'NVDA', 'PYPL', 'ADBE', 'INTC']\n",
    "\n",
    "# Function to generate random dates within a range\n",
    "def random_date(start, end):\n",
    "    return start + timedelta(\n",
    "        seconds=random.randint(0, int((end - start).total_seconds()))\n",
    "    )\n",
    "\n",
    "# Generate the CSV file\n",
    "def generate_stock_transactions_csv(filename, num_entries=500):\n",
    "    start_date = datetime(2022, 1, 1)\n",
    "    end_date = datetime(2023, 1, 1)\n",
    "\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Transaction ID', 'Stock Symbol', 'Transaction Type', 'Quantity', 'Price', 'Transaction Date'])\n",
    "\n",
    "        for i in range(1, num_entries + 1):\n",
    "            transaction_id = i\n",
    "            stock_symbol = random.choice(stock_symbols)\n",
    "            transaction_type = random.choice(['Buy', 'Sell'])\n",
    "            quantity = random.randint(1, 1000)\n",
    "            price = round(random.uniform(10.0, 1000.0), 2)\n",
    "            transaction_date = random_date(start_date, end_date).strftime('%Y-%m-%d')\n",
    "\n",
    "            writer.writerow([transaction_id, stock_symbol, transaction_type, quantity, price, transaction_date])\n",
    "\n",
    "# Generate the CSV file with 500 entries\n",
    "generate_stock_transactions_csv('stock_transactions.csv', 500)\n",
    "print(\"CSV file 'stock_transactions.csv' generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: infosys.csv\n",
      "   Unnamed: 0                 date  1. open  2. high  3. low  4. close  \\\n",
      "0           0  2025-01-02 04:00:00    22.71    22.90   22.71     22.90   \n",
      "1           1  2025-01-02 04:01:00    22.79    22.79   22.78     22.78   \n",
      "2           2  2025-01-02 04:02:00    22.79    22.84   22.79     22.84   \n",
      "3           3  2025-01-02 04:03:00    22.81    22.81   22.81     22.81   \n",
      "4           4  2025-01-02 04:05:00    22.84    22.84   22.83     22.83   \n",
      "\n",
      "   5. volume        EMA  Volume_Oscillator        RSI          %K          %D  \\\n",
      "0      944.0  22.900000           0.000000  50.219276  100.000000  100.000000   \n",
      "1      120.0  22.860000         -77.443609   0.000000   36.842105   68.421053   \n",
      "2       73.0  22.853333         -80.738786  33.333333   68.421053   68.421053   \n",
      "3        3.0  22.838889         -98.947368  28.571429   52.631579   52.631579   \n",
      "4      392.0  22.835926          27.937337  34.782609   63.157895   61.403509   \n",
      "\n",
      "         +DI        -DI        ADX           PVT    Target  Comprehensive_RRR  \\\n",
      "0  22.653347  24.242424   30.58053 -15111.818938  0.000000          -0.020242   \n",
      "1   0.000000   0.000000   30.58053     -0.628821 -0.005240          -0.020242   \n",
      "2  27.777778   0.000000  100.00000     -0.436547  0.002634          -0.020242   \n",
      "3  23.809524   0.000000  100.00000     -0.440487 -0.001313          -0.020242   \n",
      "4  33.333333   0.000000  100.00000     -0.096779  0.000877          -0.020242   \n",
      "\n",
      "   %Change       RRR  \n",
      "0 -3.43461  0.256545  \n",
      "1 -3.43461  0.256545  \n",
      "2 -3.43461  0.256545  \n",
      "3 -3.43461  0.256545  \n",
      "4 -3.43461  0.256545  \n",
      "Processing file: reliance.csv\n",
      "   Unnamed: 0                 date  1. open  2. high  3. low  4. close  \\\n",
      "0           0  2024-12-30 04:00:00     2.36     2.36    2.36      2.36   \n",
      "1           1  2024-12-30 04:01:00     2.36     2.36    2.36      2.36   \n",
      "2           2  2024-12-30 04:02:00     2.38     2.38    2.38      2.38   \n",
      "3           3  2024-12-30 04:04:00     2.46     2.46    2.46      2.46   \n",
      "4           4  2024-12-30 04:12:00     2.38     2.47    2.38      2.47   \n",
      "\n",
      "   5. volume       EMA  Volume_Oscillator         RSI          %K          %D  \\\n",
      "0       10.0  2.360000           0.000000   49.219129   43.196544   42.953016   \n",
      "1        1.0  2.360000         -81.818182   49.219129   43.196544   42.953016   \n",
      "2        3.0  2.366667         -35.714286  100.000000  100.000000  100.000000   \n",
      "3        2.0  2.397778         -50.000000  100.000000  100.000000  100.000000   \n",
      "4      126.0  2.421852         343.661972  100.000000  100.000000  100.000000   \n",
      "\n",
      "          +DI        -DI         ADX         PVT    Target  Comprehensive_RRR  \\\n",
      "0   40.810811  41.937054   17.661816  605.529682  0.000000           0.000968   \n",
      "1   40.810811  41.937054   17.661816    0.000000  0.000000           0.000968   \n",
      "2  100.000000   0.000000  100.000000    0.025424  0.008475           0.000968   \n",
      "3  100.000000   0.000000  100.000000    0.092651  0.033613           0.000968   \n",
      "4   52.631579  42.105263   70.370370    0.604846  0.004065           0.000968   \n",
      "\n",
      "    %Change       RRR  \n",
      "0 -17.79661  1.242857  \n",
      "1 -17.79661  1.242857  \n",
      "2 -17.79661  1.242857  \n",
      "3 -17.79661  1.242857  \n",
      "4 -17.79661  1.242857  \n",
      "Suggested Trading Strategy:\n",
      "Long-term investment in stable stocks.\n",
      "\n",
      "Matching CSV Files:\n",
      "Filename: infosys, Features: [{'%Change': -3.4346103038309166, 'RRR': 0.2565445026178002}, {'%Change': -3.4346103038309166, 'RRR': 0.2565445026178002}, {'%Change': -3.4346103038309166, 'RRR': 0.2565445026178002}, {'%Change': -3.4346103038309166, 'RRR': 0.2565445026178002}, {'%Change': -3.4346103038309166, 'RRR': 0.2565445026178002}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def suggest_trading_strategy(user_risk_tolerance, folder_path):\n",
    "    \"\"\"\n",
    "    Suggests a trading strategy based on user risk tolerance and stock data from multiple CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - user_risk_tolerance (str): The user's risk tolerance ('low', 'medium', 'high').\n",
    "    - folder_path (str): The path to the folder containing CSV files with stock data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the suggested trading strategy and matching CSV file names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list to hold names of matching CSV files and their features\n",
    "    matching_files_info = []\n",
    "\n",
    "    # Loop through all CSV files in the specified folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            stock_data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Rename columns for easier access\n",
    "            stock_data.columns = [col.strip() for col in stock_data.columns]  # Strip any whitespace from column names\n",
    "            \n",
    "            # Display the first few rows of each DataFrame for inspection (optional)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            print(stock_data.head())\n",
    "\n",
    "            # Define trading strategy based on user risk tolerance\n",
    "            if user_risk_tolerance == 'low':\n",
    "                filtered_stocks = stock_data[(stock_data['%Change'].abs() < 5) & \n",
    "                                              (stock_data['RRR'] < 0.4)]\n",
    "                strategy = \"Long-term investment in stable stocks.\"\n",
    "                \n",
    "            elif user_risk_tolerance == 'medium':\n",
    "                filtered_stocks = stock_data[(stock_data['%Change'].abs() >= 5) & \n",
    "                                              (stock_data['%Change'].abs() < 10) &\n",
    "                                              (stock_data['RRR'] >= 0.4) & \n",
    "                                              (stock_data['RRR'] < 1)]\n",
    "                strategy = \"Balanced portfolio with a mix of growth and income stocks.\"\n",
    "                \n",
    "            elif user_risk_tolerance == 'high':\n",
    "                filtered_stocks = stock_data[(stock_data['%Change'].abs() >= 10) & \n",
    "                                              (stock_data['RRR'] >= 1)]\n",
    "                strategy = \"Short-term trading in high-risk stocks.\"\n",
    "                \n",
    "            else:\n",
    "                return {\"error\": \"Invalid risk tolerance level. Please choose 'low', 'medium', or 'high'.\"}\n",
    "            \n",
    "            # If there are any filtered stocks, add the filename without extension to the list\n",
    "            if not filtered_stocks.empty:\n",
    "                matching_files_info.append({\n",
    "                    \"filename\": os.path.splitext(filename)[0],  # Get filename without extension\n",
    "                    \"features\": filtered_stocks[['%Change', 'RRR']].head().to_dict(orient='records')  # Get features of first few matching stocks\n",
    "                })\n",
    "\n",
    "    # Return the suggested trading strategy and matching files info\n",
    "    return {\n",
    "        \"strategy\": strategy,\n",
    "        \"matching_files\": matching_files_info\n",
    "    }\n",
    "\n",
    "# Example usage of the function\n",
    "user_risk_tolerance = 'low'  # User-defined risk tolerance\n",
    "folder_path = r'C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\dataset_testing'  # Path to your folder containing CSV files\n",
    "\n",
    "# Call the function\n",
    "trading_suggestion = suggest_trading_strategy(user_risk_tolerance, folder_path)\n",
    "\n",
    "# Display the suggested strategy and matching files\n",
    "if \"error\" in trading_suggestion:\n",
    "    print(trading_suggestion[\"error\"])\n",
    "else:\n",
    "    print(\"Suggested Trading Strategy:\")\n",
    "    print(trading_suggestion['strategy'])\n",
    "    print(\"\\nMatching CSV Files:\")\n",
    "    for file_info in trading_suggestion['matching_files']:\n",
    "        print(f\"Filename: {file_info['filename']}, Features: {file_info['features']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined Risk Tolerance Level: high\n",
      "Processing file: infosys.csv\n",
      "   Unnamed: 0                 date  1. open  2. high  3. low  4. close  \\\n",
      "0           0  2025-01-02 04:00:00    22.71    22.90   22.71     22.90   \n",
      "1           1  2025-01-02 04:01:00    22.79    22.79   22.78     22.78   \n",
      "2           2  2025-01-02 04:02:00    22.79    22.84   22.79     22.84   \n",
      "3           3  2025-01-02 04:03:00    22.81    22.81   22.81     22.81   \n",
      "4           4  2025-01-02 04:05:00    22.84    22.84   22.83     22.83   \n",
      "\n",
      "   5. volume        EMA  Volume_Oscillator        RSI          %K          %D  \\\n",
      "0      944.0  22.900000           0.000000  50.219276  100.000000  100.000000   \n",
      "1      120.0  22.860000         -77.443609   0.000000   36.842105   68.421053   \n",
      "2       73.0  22.853333         -80.738786  33.333333   68.421053   68.421053   \n",
      "3        3.0  22.838889         -98.947368  28.571429   52.631579   52.631579   \n",
      "4      392.0  22.835926          27.937337  34.782609   63.157895   61.403509   \n",
      "\n",
      "         +DI        -DI        ADX           PVT    Target  Comprehensive_RRR  \\\n",
      "0  22.653347  24.242424   30.58053 -15111.818938  0.000000          -0.020242   \n",
      "1   0.000000   0.000000   30.58053     -0.628821 -0.005240          -0.020242   \n",
      "2  27.777778   0.000000  100.00000     -0.436547  0.002634          -0.020242   \n",
      "3  23.809524   0.000000  100.00000     -0.440487 -0.001313          -0.020242   \n",
      "4  33.333333   0.000000  100.00000     -0.096779  0.000877          -0.020242   \n",
      "\n",
      "   %Change       RRR  \n",
      "0 -3.43461  0.256545  \n",
      "1 -3.43461  0.256545  \n",
      "2 -3.43461  0.256545  \n",
      "3 -3.43461  0.256545  \n",
      "4 -3.43461  0.256545  \n",
      "Processing file: reliance.csv\n",
      "   Unnamed: 0                 date  1. open  2. high  3. low  4. close  \\\n",
      "0           0  2024-12-30 04:00:00     2.36     2.36    2.36      2.36   \n",
      "1           1  2024-12-30 04:01:00     2.36     2.36    2.36      2.36   \n",
      "2           2  2024-12-30 04:02:00     2.38     2.38    2.38      2.38   \n",
      "3           3  2024-12-30 04:04:00     2.46     2.46    2.46      2.46   \n",
      "4           4  2024-12-30 04:12:00     2.38     2.47    2.38      2.47   \n",
      "\n",
      "   5. volume       EMA  Volume_Oscillator         RSI          %K          %D  \\\n",
      "0       10.0  2.360000           0.000000   49.219129   43.196544   42.953016   \n",
      "1        1.0  2.360000         -81.818182   49.219129   43.196544   42.953016   \n",
      "2        3.0  2.366667         -35.714286  100.000000  100.000000  100.000000   \n",
      "3        2.0  2.397778         -50.000000  100.000000  100.000000  100.000000   \n",
      "4      126.0  2.421852         343.661972  100.000000  100.000000  100.000000   \n",
      "\n",
      "          +DI        -DI         ADX         PVT    Target  Comprehensive_RRR  \\\n",
      "0   40.810811  41.937054   17.661816  605.529682  0.000000           0.000968   \n",
      "1   40.810811  41.937054   17.661816    0.000000  0.000000           0.000968   \n",
      "2  100.000000   0.000000  100.000000    0.025424  0.008475           0.000968   \n",
      "3  100.000000   0.000000  100.000000    0.092651  0.033613           0.000968   \n",
      "4   52.631579  42.105263   70.370370    0.604846  0.004065           0.000968   \n",
      "\n",
      "    %Change       RRR  \n",
      "0 -17.79661  1.242857  \n",
      "1 -17.79661  1.242857  \n",
      "2 -17.79661  1.242857  \n",
      "3 -17.79661  1.242857  \n",
      "4 -17.79661  1.242857  \n",
      "Suggested Trading Strategy:\n",
      "Short-term trading in high-risk stocks.\n",
      "\n",
      "Matching CSV Files:\n",
      "Filename: reliance, Features: [{'%Change': -17.796610169491522, 'RRR': 1.242857142857143}, {'%Change': -17.796610169491522, 'RRR': 1.242857142857143}, {'%Change': -17.796610169491522, 'RRR': 1.242857142857143}, {'%Change': -17.796610169491522, 'RRR': 1.242857142857143}, {'%Change': -17.796610169491522, 'RRR': 1.242857142857143}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r'C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\stock_transactions.csv'  # Update this path as needed\n",
    "stock_data = pd.read_csv(file_path)\n",
    "\n",
    "# Rename columns for easier access\n",
    "stock_data.columns = [col.strip() for col in stock_data.columns]\n",
    "\n",
    "# Define weights and biases for each stock symbol (these are arbitrary values for demonstration)\n",
    "weights = {\n",
    "    'AAPL': (0.3, 0.5),   # (weight, bias)\n",
    "    'GOOGL': (0.4, 0.6),\n",
    "    'AMZN': (0.5, 0.7),\n",
    "    'MSFT': (0.3, 0.4),\n",
    "    'TSLA': (0.6, 0.8),\n",
    "    'FB': (0.4, 0.5),\n",
    "    'NVDA': (0.5, 0.6),\n",
    "    'PYPL': (0.3, 0.4),\n",
    "    'ADBE': (0.4, 0.5),\n",
    "    'INTC': (0.3, 0.4)\n",
    "}\n",
    "\n",
    "# Initialize variables to store total weighted scores and counts\n",
    "total_weighted_score = 0\n",
    "total_count = 0\n",
    "\n",
    "# Calculate weighted scores based on transactions\n",
    "for index, row in stock_data.iterrows():\n",
    "    stock_symbol = row['Stock Symbol']\n",
    "    quantity = row['Quantity']\n",
    "    \n",
    "    if stock_symbol in weights:\n",
    "        weight, bias = weights[stock_symbol]\n",
    "        # Calculate score based on transaction\n",
    "        score = weight * quantity + bias\n",
    "        total_weighted_score += score\n",
    "        total_count += 1\n",
    "\n",
    "# Determine risk tolerance level based on total weighted score\n",
    "if total_count > 0:\n",
    "    average_score = total_weighted_score / total_count\n",
    "    \n",
    "    if average_score < 50:\n",
    "        risk_tolerance = 'low'\n",
    "    elif 50 <= average_score < 100:\n",
    "        risk_tolerance = 'medium'\n",
    "    else:\n",
    "        risk_tolerance = 'high'\n",
    "else:\n",
    "    risk_tolerance = 'unknown'\n",
    "\n",
    "# Store the risk tolerance level in a variable\n",
    "print(f\"Determined Risk Tolerance Level: {risk_tolerance}\")\n",
    "\n",
    "# Now you can pass this variable to your trading strategy function\n",
    "folder_path = r'C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\dataset_testing'  # Path to your folder containing CSV files\n",
    "\n",
    "# Call the function with determined risk tolerance\n",
    "trading_suggestion = suggest_trading_strategy(risk_tolerance, folder_path)\n",
    "\n",
    "# Display the suggested strategy and matching files\n",
    "if \"error\" in trading_suggestion:\n",
    "    print(trading_suggestion[\"error\"])\n",
    "else:\n",
    "    print(\"Suggested Trading Strategy:\")\n",
    "    print(trading_suggestion['strategy'])\n",
    "    print(\"\\nMatching CSV Files:\")\n",
    "    for file_info in trading_suggestion['matching_files']:\n",
    "        print(f\"Filename: {file_info['filename']}, Features: {file_info['features']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-27 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-27 04:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-27 05:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-27 05:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-27 05:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>2025-01-31 18:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>2025-01-31 18:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>2025-01-31 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>2025-01-31 19:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>2025-01-31 19:53:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2170 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Datetime\n",
       "0     2025-01-27 04:00:00\n",
       "1     2025-01-27 04:57:00\n",
       "2     2025-01-27 05:03:00\n",
       "3     2025-01-27 05:36:00\n",
       "4     2025-01-27 05:52:00\n",
       "...                   ...\n",
       "2165  2025-01-31 18:36:00\n",
       "2166  2025-01-31 18:59:00\n",
       "2167  2025-01-31 19:00:00\n",
       "2168  2025-01-31 19:39:00\n",
       "2169  2025-01-31 19:53:00\n",
       "\n",
       "[2170 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail = pd.read_csv(r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\dates.csv\")\n",
    "detail\n",
    "detail = detail.drop(columns=['Unnamed: 0'])\n",
    "detail.to_csv(\"dates.csv\")\n",
    "detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified prediction CSV saved as: C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the prediction CSV file\n",
    "prediction_file_path = r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\predictions.csv\"  # Update this path as needed\n",
    "prediction_data = pd.read_csv(prediction_file_path, header=None)  # Load without headers\n",
    "\n",
    "# Change column names to 'Target' and 'Comprehensive RRR'\n",
    "prediction_data.columns = ['Target', 'Comprehensive RRR']\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "modified_prediction_file_path = r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\predictions.csv\"  # Specify new file path if needed\n",
    "prediction_data.to_csv(modified_prediction_file_path, index=False)\n",
    "\n",
    "print(f\"Modified prediction CSV saved as: {modified_prediction_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Comprehensive RRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000653</td>\n",
       "      <td>-0.061954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000150</td>\n",
       "      <td>-0.025017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.015324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.016014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000175</td>\n",
       "      <td>-0.006348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>-0.002273</td>\n",
       "      <td>-0.003638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>0.000933</td>\n",
       "      <td>-0.001217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>-0.000646</td>\n",
       "      <td>-0.001167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>-0.000204</td>\n",
       "      <td>0.000872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>-0.002074</td>\n",
       "      <td>-0.003303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2170 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target  Comprehensive RRR\n",
       "0     0.000653          -0.061954\n",
       "1    -0.000150          -0.025017\n",
       "2    -0.000195          -0.015324\n",
       "3    -0.000040          -0.016014\n",
       "4    -0.000175          -0.006348\n",
       "...        ...                ...\n",
       "2165 -0.002273          -0.003638\n",
       "2166  0.000933          -0.001217\n",
       "2167 -0.000646          -0.001167\n",
       "2168 -0.000204           0.000872\n",
       "2169 -0.002074          -0.003303\n",
       "\n",
       "[2170 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.read_csv(r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\predictions.csv\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'INFOSYS': (0      2025-01-27 04:00:00\n",
      "1      2025-01-27 04:57:00\n",
      "2      2025-01-27 05:03:00\n",
      "3      2025-01-27 05:36:00\n",
      "4      2025-01-27 05:52:00\n",
      "               ...        \n",
      "2165   2025-01-31 18:36:00\n",
      "2166   2025-01-31 18:59:00\n",
      "2167   2025-01-31 19:00:00\n",
      "2168   2025-01-31 19:39:00\n",
      "2169   2025-01-31 19:53:00\n",
      "Name: Datetime, Length: 2170, dtype: datetime64[ns], array([[ 0.00065273, -0.06195421],\n",
      "       [-0.00014951, -0.02501749],\n",
      "       [-0.00019475, -0.01532434],\n",
      "       ...,\n",
      "       [-0.00064619, -0.00116681],\n",
      "       [-0.00020362,  0.00087188],\n",
      "       [-0.00207374, -0.00330309]], shape=(2170, 2)))}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to hold stock data\n",
    "stock_data = {}\n",
    "\n",
    "# Assuming we have a known stock ticker for demonstration; you can modify this as needed\n",
    "ticker = 'INFOSYS'  # This can be any stock symbol you choose\n",
    "\n",
    "# Create a date range from the dates CSV\n",
    "date_range = pd.to_datetime(detail['Datetime'])\n",
    "\n",
    "# Convert prediction data to a NumPy array\n",
    "predictions_array = prediction_data.to_numpy()\n",
    "\n",
    "# Store in the dictionary\n",
    "stock_data[ticker] = (date_range, predictions_array)\n",
    "\n",
    "# Display the resulting dictionary structure\n",
    "print(stock_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
