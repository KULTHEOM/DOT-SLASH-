{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Begin amigos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define Classes for News Scraping\n",
    "class NewsArticle:\n",
    "    def __init__(self, title: str, content: str, date: str, source: str, url: str):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.date = date\n",
    "        self.source = source\n",
    "        self.url = url\n",
    "\n",
    "class FinancialNewsScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        self.sources = {\n",
    "            'reuters': 'https://www.reuters.com/markets/companies',\n",
    "            'marketwatch': 'https://www.marketwatch.com/markets',\n",
    "            'investing': 'https://www.investing.com/news/stock-market-news'\n",
    "        }\n",
    "\n",
    "    def scrape_news(self, days_back: int = 7) -> List[NewsArticle]:\n",
    "        articles = []\n",
    "        for source, url in self.sources.items():\n",
    "            try:\n",
    "                articles.extend(self._scrape_source(source, url, days_back))\n",
    "                time.sleep(2)  # Polite delay between sources\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping {source}: {str(e)}\")\n",
    "        return articles\n",
    "\n",
    "    def _scrape_source(self, source: str, url: str, days_back: int) -> List[NewsArticle]:\n",
    "        articles = []\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            if source == 'reuters':\n",
    "                articles.extend(self._parse_reuters(soup))\n",
    "            elif source == 'marketwatch':\n",
    "                articles.extend(self._parse_marketwatch(soup))\n",
    "            elif source == 'investing':\n",
    "                articles.extend(self._parse_investing(soup))\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in _scrape_source for {source}: {str(e)}\")\n",
    "            \n",
    "        return articles\n",
    "\n",
    "    def _parse_reuters(self, soup: BeautifulSoup) -> List[NewsArticle]:\n",
    "        articles = []\n",
    "        for article in soup.find_all('article'):\n",
    "            try:\n",
    "                title = article.find('h3').text.strip()\n",
    "                link = article.find('a')['href']\n",
    "                article_content = self._get_article_content(f\"https://reuters.com{link}\")\n",
    "                date = datetime.now().strftime('%Y-%m-%d')  # Reuters articles usually have current date\n",
    "                articles.append(NewsArticle(title, article_content, date, 'Reuters', link))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error parsing Reuters article: {str(e)}\")\n",
    "        return articles\n",
    "\n",
    "    def _get_article_content(self, url: str) -> str:\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            return ' '.join([p.text.strip() for p in paragraphs])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting article content: {str(e)}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Classes for Stock Prediction\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, sequence_length: int, target_column: int):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.sequence_length]\n",
    "        y = self.data[idx + self.sequence_length][self.target_column]\n",
    "        return x, y\n",
    "\n",
    "class StockPredictor:\n",
    "    def __init__(self,\n",
    "                 time_series_model_name: str = \"huggingface/time-series-transformer\",\n",
    "                 sentiment_model_name: str = \"ProsusAI/finbert\"):\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.time_series_model = AutoModel.from_pretrained(time_series_model_name).to(self.device)\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name).to(self.device)\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "        \n",
    "        # Initialize scaler and news scraper\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.news_scraper = FinancialNewsScraper()\n",
    "\n",
    "    def load_data_from_csv(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load stock data from a CSV file.\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure that the Date column is in datetime format and set it as index if necessary.\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "        \n",
    "        # Calculate technical indicators (if needed)\n",
    "        df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
    "        df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "        \n",
    "        return df.dropna()\n",
    "\n",
    "    def prepare_data_for_training(self, df: pd.DataFrame, sequence_length: int = 30) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Prepare data for model training\"\"\"\n",
    "        \n",
    "        # Scale the data\n",
    "        scaled_data = self.scaler.fit_transform(df)\n",
    "\n",
    "        # Create dataset\n",
    "        dataset = FinancialDataset(\n",
    "        data=scaled_data,\n",
    "        sequence_length=sequence_length,\n",
    "        target_column=df.columns.get_loc('Close')\n",
    "        )\n",
    "\n",
    "         # Split into train and validation sets\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, len(dataset) - train_size]\n",
    "        )\n",
    "         \n",
    "         # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train_model(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 10):\n",
    "        \"\"\"Train the time series model\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.time_series_model.parameters())\n",
    "        criterion = torch.nn.MSELoss()\n",
    "         \n",
    "        for epoch in range(epochs):\n",
    "            # Training loop\n",
    "            self.time_series_model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.time_series_model(batch_x)\n",
    "                loss = criterion(outputs.squeeze(), batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader)}')\n",
    "\n",
    "    def analyze_sentiment(self, text: str) -> Tuple[int, np.ndarray]:\n",
    "        \"\"\"Analyze sentiment of financial news\"\"\"\n",
    "        inputs = self.sentiment_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.sentiment_model(**inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            prediction = torch.argmax(probabilities).item()\n",
    "\n",
    "        return prediction, probabilities.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Scrape News Articles (Run in Parallel)\n",
    "news_scraper = FinancialNewsScraper()\n",
    "news_articles = news_scraper.scrape_news(days_back=7)\n",
    "\n",
    "# Display the scraped news titles and sources.\n",
    "for article in news_articles:\n",
    "    print(f\"{article.date} - {article.source}: {article.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Stock Data from CSV and Train Model (Run in Parallel)\n",
    "file_path_to_csv = 'path/to/your/stock_data.csv'  # Update this path to your CSV file.\n",
    "\n",
    "predictor = StockPredictor()\n",
    "df_stock_data = predictor.load_data_from_csv(file_path_to_csv)\n",
    "\n",
    "# Prepare data loaders using the loaded data.\n",
    "train_loader, val_loader = predictor.prepare_data_for_training(df_stock_data)\n",
    "\n",
    "# Train the model.\n",
    "print(\"Training model...\")\n",
    "predictor.train_model(train_loader=train_loader,\n",
    "                      val_loader=val_loader,\n",
    "                      epochs=10)  # Set epochs as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Make Predictions with Sentiment Analysis (Combine Results)\n",
    "def predict_stock_price_with_sentiment(predictor: StockPredictor,\n",
    "                                        df_stock_data: pd.DataFrame,\n",
    "                                        news_articles: List[NewsArticle]) -> Dict:\n",
    "    \n",
    "    \"\"\"Make stock price predictions considering both technical and sentiment analysis.\"\"\"\n",
    "    \n",
    "    # Prepare input sequence from latest stock data.\n",
    "    sequence_length = 30\n",
    "    scaled_data = predictor.scaler.transform(df_stock_data.values)\n",
    "    input_sequence = torch.FloatTensor(scaled_data[-sequence_length:]).unsqueeze(0).to(predictor.device)\n",
    "\n",
    "    # Make prediction using the time series model.\n",
    "    predictor.time_series_model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction_output = predictor.time_series_model(input_sequence)\n",
    "\n",
    "    predicted_price_scaled = prediction_output.cpu().numpy()[0][0]\n",
    "\n",
    "    # Analyze sentiment from news articles.\n",
    "    sentiment_scores=[]\n",
    "    \n",
    "    for article in news_articles:\n",
    "       sentiment_score,_= predictor.analyze_sentiment(article.title + \" \" + article.content)\n",
    "       sentiment_scores.append(sentiment_score)\n",
    "\n",
    "    average_sentiment_score=np.mean(sentiment_scores) if sentiment_scores else 0\n",
    "    \n",
    "   # Adjust prediction based on sentiment.\n",
    "    predicted_price_adjusted= predicted_price_scaled * (1 + 0.01 * (average_sentiment_score - 1))\n",
    "    \n",
    "    return {\n",
    "       'predicted_price': predicted_price_adjusted,\n",
    "       'sentiment_score': average_sentiment_score,\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage of predicting price along with sentiment analysis.\n",
    "prediction_results=predict_stock_price_with_sentiment(predictor=predictor,\n",
    "                                                      df_stock_data=df_stock_data,\n",
    "                                                      news_articles=news_articles)\n",
    "\n",
    "print(f\"\\nPrediction Results:\")\n",
    "print(f\"Predicted Price: ${prediction_results['predicted_price']:.2f}\")\n",
    "print(f\"Average Sentiment Score: {prediction_results['sentiment_score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
