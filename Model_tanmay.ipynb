{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Begin amigos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsArticle:\n",
    "    def __init__(self, title: str, content: str, date: str, source: str, url: str):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.date = date\n",
    "        self.source = source\n",
    "        self.url = url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialNewsScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        self.sources = {\n",
    "            'reuters': 'https://www.reuters.com/markets/companies',\n",
    "            'marketwatch': 'https://www.marketwatch.com/markets',\n",
    "            'investing': 'https://www.investing.com/news/stock-market-news'\n",
    "        }\n",
    "\n",
    "    def scrape_news(self, days_back: int = 7) -> List[NewsArticle]:\n",
    "        articles = []\n",
    "        for source, url in self.sources.items():\n",
    "            try:\n",
    "                articles.extend(self._scrape_source(source, url, days_back))\n",
    "                time.sleep(2)  # Polite delay between sources\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping {source}: {str(e)}\")\n",
    "        return articles\n",
    "\n",
    "    def _scrape_source(self, source: str, url: str, days_back: int) -> List[NewsArticle]:\n",
    "        articles = []\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            if source == 'reuters':\n",
    "                articles.extend(self._parse_reuters(soup))\n",
    "            elif source == 'marketwatch':\n",
    "                articles.extend(self._parse_marketwatch(soup))\n",
    "            elif source == 'investing':\n",
    "                articles.extend(self._parse_investing(soup))\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in _scrape_source for {source}: {str(e)}\")\n",
    "            \n",
    "        return articles\n",
    "\n",
    "    def _parse_reuters(self, soup: BeautifulSoup) -> List[NewsArticle]:\n",
    "        articles = []\n",
    "        for article in soup.find_all('article'):\n",
    "            try:\n",
    "                title = article.find('h3').text.strip()\n",
    "                link = article.find('a')['href']\n",
    "                article_content = self._get_article_content(f\"https://reuters.com{link}\")\n",
    "                date = datetime.now().strftime('%Y-%m-%d')  # Reuters articles usually have current date\n",
    "                articles.append(NewsArticle(title, article_content, date, 'Reuters', link))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error parsing Reuters article: {str(e)}\")\n",
    "        return articles\n",
    "\n",
    "    def _get_article_content(self, url: str) -> str:\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            return ' '.join([p.text.strip() for p in paragraphs])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting article content: {str(e)}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, sequence_length: int, target_column: int):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_column = target_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.sequence_length]\n",
    "        y = self.data[idx + self.sequence_length][self.target_column]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class StockPredictor:\n",
    "    def __init__(self,\n",
    "                 time_series_model_name: str = \"huggingface/TimeSeriesTransformer\",\n",
    "                 sentiment_model_name: str = \"ProsusAI/finbert\",\n",
    "                 api_access_token: str = \"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\"):\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load time series model for regression\n",
    "        self.time_series_model = AutoModel.from_pretrained(time_series_model_name, token=api_access_token).to(self.device)\n",
    "\n",
    "        # Load sentiment analysis model\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name, token=api_access_token).to(self.device)\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name, token=api_access_token)\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def load_data_from_csv(self, file_path: str):\n",
    "        \"\"\"Load stock data from a CSV file and split into features (X) and target (y)\"\"\"\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Standardize column names (remove leading/trailing spaces)\n",
    "            df.columns = df.columns.str.strip()\n",
    "\n",
    "            # Expected columns\n",
    "            expected_columns = [\n",
    "                \"date\", \"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\",\n",
    "                \"EMA\", \"Volume_Oscillator\", \"RSI\", \"%K\", \"%D\", \"+DI\", \"-DI\", \"ADX\", \"PVT\", \"Target\"\n",
    "            ]\n",
    "\n",
    "            missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in CSV file: {missing_cols}\")\n",
    "\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "            df.dropna(subset=[\"date\"], inplace=True)\n",
    "            df.set_index(\"date\", inplace=True)\n",
    "\n",
    "            df[expected_columns[1:]] = df[expected_columns[1:]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "            X = df.drop(columns=[\"Target\"]).values  # Features\n",
    "            y = df[\"Target\"].values  # Target variable\n",
    "\n",
    "            return X, y\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def prepare_data_for_training(self, X, y, test_size=0.2, batch_size=32):\n",
    "        \"\"\"Prepare PyTorch data loaders\"\"\"\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Normalize features\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val = self.scaler.transform(X_val)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train_model(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 10):\n",
    "        \"\"\"Train the transformer model for time series regression\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.time_series_model.parameters(), lr=1e-4)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.time_series_model.train()\n",
    "            train_loss = 0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                batch_x, batch_y = batch\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.time_series_model(batch_x).logits  # Ensure model outputs logits\n",
    "                outputs = outputs.squeeze()\n",
    "\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "    def analyze_sentiment(self, text: str):\n",
    "        \"\"\"Analyze sentiment of financial news\"\"\"\n",
    "        inputs = self.sentiment_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.sentiment_model(**inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            prediction = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "        return prediction.item(), probabilities.cpu().numpy()\n",
    "\n",
    "    def predict_stock_price(self, X):\n",
    "        \"\"\"Predict stock price using trained model\"\"\"\n",
    "        self.time_series_model.eval()\n",
    "\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.time_series_model(X_tensor).logits.squeeze().cpu().numpy()\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    \n",
    "    predictor_file_path = r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\features.csv\"\n",
    "    predictor = StockPredictor()\n",
    "\n",
    "    try:\n",
    "        # Load stock data\n",
    "        X, y = predictor.load_data_from_csv(predictor_file_path)\n",
    "\n",
    "        if X is None or y is None:\n",
    "            logging.error(\"Failed to load data.\")\n",
    "            return\n",
    "\n",
    "        # Prepare data loaders\n",
    "        train_loader, val_loader = predictor.prepare_data_for_training(X, y)\n",
    "\n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        predictor.train_model(train_loader, val_loader)\n",
    "\n",
    "        # Predict stock price\n",
    "        X_test = X[-10:]  # Example: last 10 rows for prediction\n",
    "        predictions = predictor.predict_stock_price(X_test)\n",
    "\n",
    "        print(f\"\\nPredicted Stock Prices: {predictions}\")\n",
    "\n",
    "        # Sentiment analysis example\n",
    "        sentiment_result = predictor.analyze_sentiment(\"The stock market is experiencing a strong bullish trend.\")\n",
    "        print(f\"\\nSentiment Analysis: {sentiment_result}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {str(e)}\")\n",
    "        print(\"An error occurred. Check logs for details.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "class StockPredictor(nn.Module):\n",
    "    def __init__(self, time_series_model_name: str = \"bert-base-uncased\", api_access_token: str = \"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\"):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load transformer model pre-trained for sequence classification (can be used for regression tasks)\n",
    "        self.time_series_model = AutoModelForSequenceClassification.from_pretrained(time_series_model_name, \n",
    "                                                                                    num_labels=1).to(self.device)\n",
    "\n",
    "        # Update the fully connected layer to match your 17 input features\n",
    "        self.fc = nn.Linear(17, 768)  # Change 15 to 17 to match your input features\n",
    "\n",
    "        # Initialize scaler for normalization\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Pass through the fully connected layer to reshape the input into the transformer input format\n",
    "        X = self.fc(X)  # Shape: [batch_size, 768]\n",
    "\n",
    "        # Reshape to match transformer input: [batch_size, 1, 768]\n",
    "        X = X.unsqueeze(1)  # Adding sequence dimension, so the shape becomes [batch_size, seq_length=1, 768]\n",
    "\n",
    "        # Pass through the transformer model\n",
    "        outputs = self.time_series_model(inputs_embeds=X).logits\n",
    "        return outputs\n",
    "\n",
    "    def load_data_from_csv(self, file_path: str):\n",
    "        \"\"\"Load stock data from a CSV file and split into features (X) and target (y)\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Standardize column names (remove leading/trailing spaces)\n",
    "            df.columns = df.columns.str.strip()\n",
    "\n",
    "            # Expected columns\n",
    "            expected_columns = [\n",
    "                \"date\", \"1. open\", \"2. high\", \"3. low\", \"4. close\", \"5. volume\",\n",
    "                \"EMA\", \"Volume_Oscillator\", \"RSI\", \"%K\", \"%D\", \"+DI\", \"-DI\", \"ADX\", \"PVT\", \"Target\"\n",
    "            ]\n",
    "\n",
    "            missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in CSV file: {missing_cols}\")\n",
    "\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "            df.dropna(subset=[\"date\"], inplace=True)\n",
    "            df.set_index(\"date\", inplace=True)\n",
    "\n",
    "            df[expected_columns[1:]] = df[expected_columns[1:]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "            X = df.drop(columns=[\"Target\"]).values  # Features\n",
    "            y = df[\"Target\"].values  # Target variable\n",
    "\n",
    "            return X, y\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def prepare_data_for_training(self, X, y, test_size=0.2, batch_size=32):\n",
    "        \"\"\"Prepare PyTorch data loaders\"\"\"\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Normalize features\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val = self.scaler.transform(X_val)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(self.device)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train_model(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 10):\n",
    "        \"\"\"Train the transformer model for time series regression\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss = 0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                batch_x, batch_y = batch\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass through the model\n",
    "                outputs = self(batch_x).squeeze()\n",
    "\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "    def save_model(self, model_save_path: str):\n",
    "        \"\"\"Save the trained model to a .pt file\"\"\"\n",
    "        torch.save(self.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  2.36000000e+00  2.36000000e+00 ...  4.19370539e+01\n",
      "   1.76618161e+01  6.05529682e+02]\n",
      " [ 1.00000000e+00  2.36000000e+00  2.36000000e+00 ...  4.19370539e+01\n",
      "   1.76618161e+01  0.00000000e+00]\n",
      " [ 2.00000000e+00  2.38000000e+00  2.38000000e+00 ...  0.00000000e+00\n",
      "   1.00000000e+02  2.54237288e-02]\n",
      " ...\n",
      " [ 2.86000000e+03  1.99000000e+00  2.00000000e+00 ...  3.79790941e+01\n",
      "   2.12055992e+01 -1.24895400e+03]\n",
      " [ 2.86100000e+03  2.00000000e+00  2.00000000e+00 ...  4.27059844e+01\n",
      "   2.06547429e+01 -1.24940400e+03]\n",
      " [ 2.86200000e+03  1.94000000e+00  1.94000000e+00 ...  3.89012209e+01\n",
      "   2.01836100e+01 -1.24940400e+03]] [ 0.          0.          0.00847458 ...  0.03092784 -0.03\n",
      "  0.        ]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x15 and 17x768)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m stock_predictor\u001b[38;5;241m.\u001b[39mprepare_data_for_training(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mstock_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m stock_predictor\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstock_predictor_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[58], line 102\u001b[0m, in \u001b[0;36mStockPredictor.train_model\u001b[1;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    104\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m    105\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[58], line 27\u001b[0m, in \u001b[0;36mStockPredictor.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Pass through the fully connected layer before the transformer model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, 768]\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Pass through the transformer model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_series_model(inputs_embeds\u001b[38;5;241m=\u001b[39mX)\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x15 and 17x768)"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "\n",
    "stock_predictor = StockPredictor()\n",
    "\n",
    "X, y = stock_predictor.load_data_from_csv(r\"C:\\Users\\Ajay\\Desktop\\DOT-SLASH-\\features.csv\")\n",
    "print(X, y)  # Add this line to check what is being returned\n",
    "\n",
    "if X is not None and y is not None:\n",
    "    # Prepare data for training\n",
    "    train_loader, val_loader = stock_predictor.prepare_data_for_training(X, y, test_size=0.2)\n",
    "\n",
    "    # Train the model\n",
    "    stock_predictor.train_model(train_loader, val_loader, epochs=10)\n",
    "\n",
    "    # Save the trained model\n",
    "    stock_predictor.save_model(\"stock_predictor_model.pt\")\n",
    "else:\n",
    "    print(\"Error: Data loading failed. Please check your CSV file and its contents.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
